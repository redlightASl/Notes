# 常见的神经网络结构

本篇博文将全面介绍从多层感知机（MLP）到Transformer的主流神经网络结构，内容以*机器视觉*（CV）领域的算法为基本骨架，原则上按照时间顺序进行梳理，旨在梳理每篇关键论文对模型的核心改进点，并结合硬件给出量化推理/硬件化部署的基本方案。由于着重于边缘端部署，本文将不会提及视觉大模型相关内容。

## Hidden Layer和MLP

> Rumelhart D E, Hinton G E, Williams R J. Learning representations by  back-propagating errors[J]. nature, 1986, 323(6088): 533-536.

### Linear

线性层是神经网络中最基础的组件，通过矩阵乘法和偏置加法实现线性变换。其核心公式为：
$$
y(x)=Wx+b
$$
通过反向传播算法实现端到端的梯度更新，解决了早期神经网络的训练问题。但单纯的线性层无法捕捉非线性关系，需结合激活函数使用，比较常见的激活函数就是Sigmoid。

### Sigmoid

Sigmoid激活函数通过以下公式引入非线性：
$$
\sigma(x)=\frac{1}{1+e^{-x}}
$$
首次实现神经网络的非线性建模能力，但存在梯度消失问题（输入值过大或过小时导数趋近于0），因此后续被ReLU等激活函数替代。

## CNN

> LeCun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to  document recognition[J]. Proceedings of the IEEE, 1998, 86(11):  2278-2324.



### Conv-BN-Act

> AlexNet

AlexNet首次将卷积层、批归一化（BN）和激活函数（如ReLU）组合为标准模块，推动了CNN在CV领域的突破。

利用GPU并行计算卷积操作，首次将GPGPU引入机器学习-人工智能领域

使用ReLU替代Sigmoid，缓解深度神经网络中梯度消失的问题

### Shortcut

> ResNet

ResNet引入残差连接（Shortcut），解决了深度网络的梯度消失问题。通过残差旁路：
$$
y(x) = x+F(x)
$$
将上一层的梯度直接传递到下一层，从而允许训练更深的网络

### PointWise-DepthWise Conv

> MobileNetv2

mobilenet是轻量化神经网络中必须提及的。MobileNetv2继承MobileNet和ShuffleNet的思想，提出深度可分离卷积（DepthWise卷积与PointWise卷积结合），大幅减少计算量

深度可分离卷积将标准卷积分解为深度（DepthWise）卷积（逐通道卷积）和逐点（PointWise）卷积（1×1卷积），能够将网络参数减少至原来的1/9。

引入倒残差结构，即瓶颈层。先提升通道数再降维，增强网络的特征表达能力，这一点成为后续卷积神经网络的经典Trick

但近年来研究发现计算量减少伴随着访存量的增加，因此容易造成嵌入式设备侧的带宽瓶颈

### Partial Info

> CSPNet GhostNet FasterNet

CSPNet提出了跨阶段部分连接（Cross Stage Partial），将特征分为两路，一路直接传递，另一路进行计算。

GhostNet中提出通过廉价操作（如深度卷积）生成特征，减少显式参数，这使得部署时的模型参数大大减小但具有与训练时一致的精度性能。

## RNN和LSTM

> Hopfield J J, Tank D W. “Neural” computation of decisions in  optimization problems[J]. Biological cybernetics, 1985, 52(3): 141-152.
>
> Hochreiter S, Schmidhuber J. Long short-term memory[J]. Neural computation, 1997, 9(8): 1735-1780.

循环神经网络（RNN）通过循环结构处理序列数据，但存在梯度消失/爆炸问题。LSTM通过引入记忆单元（Cell State）和门控机制（输入门、遗忘门、输出门），能够有效捕捉长期依赖关系。

## GNN

> Scarselli F, Gori M, Tsoi A C, et al. The graph neural network model[J]. IEEE transactions on neural networks, 2008, 20(1): 61-80.

图神经网络（GNN）通过图结构建模节点间关系。节点通过邻居信息更新状态，适用于社交网络、分子结构等非欧几何数据

## encoder&decoder

> Cho K, Van Merriënboer B, Gulcehre C, et al. Learning phrase  representations using RNN encoder-decoder for statistical machine  translation[J]. arXiv preprint arXiv:1406.1078, 2014.

编码器-解码器结构可以用于序列到序列任务。论文提出了双向编码器用于捕捉输入序列的全局信息，这一思想成为后续Transformer诞生的前提

## GAN

> Goodfellow I, Pouget-Abadie J, Mirza M, et al. Generative adversarial  nets[J]. Advances in neural information processing systems, 2014, 27.

生成对抗网络（GAN）通过生成器和判别器的博弈生成数据。采用生成器与判别器交替优化，从而生成高质量样本

## Attention

> Bahdanau D, Cho K, Bengio Y. Neural  machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014.

注意力机制通过动态加权输入特征提升模型对关键信息的捕捉能力

## Transformer和Mamba

> Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J].  Advances in neural information processing systems, 2017, 30.

自注意力机制通过以下公式计算特征间的关联：
$$
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}}V)
$$
Transformer无需卷积的局部感受野，直接捕捉长距离依赖，同时相比RNN的串行计算，Transformer更适合GPU加速

### Embeddings

位置编码（Positional Encoding）为序列添加位置信息，从而让自然语言能够映射到数学表示空间

### Self-Attention



### Multi-Head Attention

将其拓展到多头注意力，通过多个线性变换头并行计算不同子空间的注意力

### Mamba



## 其他参考资料

https://zhuanlan.zhihu.com/p/644344950


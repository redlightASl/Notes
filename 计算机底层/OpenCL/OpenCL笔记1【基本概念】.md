# OpenCL笔记1【基本概念】

OpenCL是一个通用的异构计算模型，提供C语言接口，可以通过C扩展语法进行编程，从而实现**并行化**的**异构计算**。

> 向量化指这样一种概念：将一条指令复制多份从而在多个硬件上并行计算，以下面的C程序为例
>
> ```c
> int32_t a[4];
> int32_t b[4];
> int32_t c[4];
> //省略中间代码
> for(uint8_t i = 0; i < 4; i++)
> {
>     c[i] = a[i] + b[i];
> }
> ```
>
> 这是4个32位整数进行加法的程序
>
> 在最经典的五级流水线通用CPU上，它要执行4个时钟周期才能完成`c[i]`的计算；但对于向量化计算设备，可以将语句改写成如下形式：
>
> ```c
> _int256_m a;
> _int256_m b;
> _int256_m c;
> 
> c = a + b;
> ```
>
> 它将四个32位整数合并成256位向量，使用专用的向量加法器运算，就可以把四个周期才能完成的程序缩减成一个周期完成，从而提高计算速度

OpenCL最早由苹果公司提出，随机得到了AMD、IBM、Intel、Nvidia等公司的支持，2008年12月8日，OpenCL 1.0标准发布；2013年11月19日，OpenCL 2.0发布。这是一套通用的API，旨在减轻开发人员的编程难度，并提高程序的可移植性。目前OpenCL不仅支持Nvidia、AMD的GPGPU，也支持部分厂商的CPU、DSP、FPGA，以及任何在编译器级支持OpenCL标准的通用/向量化计算设备。

> Intel就让其大部分通用CPU设备和加速卡都支持了OpenCL，因此可以将同一份代码部署在CPU或其他Intel硬件加速卡上

## OpenCL结构简介

一个运行OpenCL程序的设备会被分成**主机**（host）和异构设备（device）两部分

OpenCL的基本架构被分成以下几部分：

* **平台模型Platform Model**：底层硬件的抽象描述，是对异构平台指令集和并行化（向量化）计算单元的封装
* **存储器模型Memory Model**：对异构平台共享/私有存储器的抽象模型，与平台模型一起为上层应用提供计算资源
* **执行模型Execution Model**：管理平台模型和存储器模型的指令框架，通过多种并行机制实现异构计算
* **编程模型Programming Model**：OpenCL的代码层**kernel**的具体实现，在主机端调用执行模型提供的资源

这四个模型相互解耦但存在依赖关系

> *Kernel就是OpenCL可执行代码的统称，也可以理解成OpenCL版本的“Cuda核函数”*
>
> Cuda核函数规定Nvidia GPU的各个线程访问哪个数据并执行什么计算
>
> Kernel就规定了某个特定设备上某个计算单元访问哪个存储器单元并执行什么计算

### 编程模型

从编程模型方面看，OpenCL有点类似于OpenMP，支持并行化的编程思想，但是它的并发单位是`work-item`，每个work-item都会执行kernel（OpenCL的代码实例），有点类似于一个硬件执行器，使用时会把这些work-item映射到底层硬件，因此它能实现比OpenMP更细粒度的并行化控制，但也更不容易控制。

### 执行模型

从执行模型方面看，OpenCL很接近某些GPU的硬件模型，它支持对硬件隔离的**上下文**机制，从而让每个kernel内部的数据处理过程不会出错。这个上下文包含了**设备抽象**、**kernel函数对象**、**程序对象**、**存储器对象**，也就是把底层的硬件资源全部抽象化为一系列具体对象，让上下文负责管理调度。其中程序对象是整个kernel程序的源代码和二进制码，比起运行在OS上的应用软件，它更像是一套动态库，在运行时由主机程序构建；kernel函数对象特指在OpenCL设备上运行的OpenCL内核函数，它才是调用OpenCL设备的运算函数。

OpenCL定义了三种Kernel：

* OpenCL内核：用OpenCL C语言编写和编译的“核函数”，对应编程模型中的具体实现
* 原生内核：在OpenCL之外创建的函数Kernel，通过一个函数指针进行调用，这些函数可以是库里封装好的函数，也可以是在其他主机源码中定义的函数，但原生内核需要在OpenCL中选定支持
* 内建内核：硬件厂商针对特定设备绑定好，不使用源码编译的函数，有点类似于现在很多WiFi基带的寄存器封装

同时OpenCL还支持**命令队列**机制，可以让内核、存储器对象分别以队列的形式在主机和设备间传输，同时支持同步命令，这就是数据流处理的思想：Kernel把指令**提交**到设备，交给设备对应的计算硬件和内存去处理，这个队列甚至允许用户自行选择顺序执行和乱序执行（需要平台支持）。在设备处理完后，再把数据通过一个缓冲队列输送回主机。为了支持更多设备，OpenCL定义了存储器模型，从而让不同存储体系结构的设备可以互相交换数据

### 存储器模型

从存储器模型方面看，OpenCL支持向量化的数据（被存入buffer对象）和矩阵化的数据（被存入image对象），但是矩阵化的数据需要依赖于硬件设备，它不像向量数组那样可以被直接引用。GPU往往可以很好地支持image运算。而buffer对象除了用于存储，还常用于主机和设备之间的数据传输。此外OpenCL还支持管道（pipe）操作，支持数据端对端传输。

> image最早用于存储标准格式的图像，但是随着科学计算需要使用计算量的提升，image支持了同步和栅栏操作，从而能对多个图像进行处理，于是在image上计算Tensor成为可能
>
> 你听说过Tensor吗？张量（Tensor）是对矩阵概念的扩展，允许比二维、三维更高的维度。
>
> 一个图像可以用三个独立的数据（更数学一点，可以称这些数据正交）表示，包含了它的宽度、长度、色彩通道；但在其他领域中（比如机器学习或科学计算），经常要处理不止一张图片或一篇文档，因此需要使用四维张量来描述数据，除了以上三个数据，还需要指定一个样本量，对于语音、有效质量乃至高维的哈密顿量这些数据，都需要更深的**维度**（dimension）去描述，张量就是一个包含了很多维度（是人为划分出来的）信息的高维矩阵
>

OpenCL定义了三种存储器区域：

* **主机内存**host memory：主机可用的内存，通过OpenCL API或共享虚拟存储器接口可以实现存储器对象在主机和设备之间的传输
* **全局存储器**global memory：上下文中任何设备、所有工作组、所有work-item都能读写的内存区域
* **常量存储器**constant memory：对于work-item只读的存储区域，只有主机在计算开始前对这片内存区域进行分配和初始化
* **私有存储器**private memory：work-item私有访问的存储器，对于其他的work-item不可见

主机和设备之间支持三种交互方式：

* **读/写/填充**：直接进行主机-设备间的数据传输
* **映射/解映射**：主机将一个存储器区域映射到可以访问的地址空间，用于安全读写缓冲区
* **拷贝**：将存储器对象在两个缓冲区间进行拷贝，这两个缓冲区是驻留在主机或设备的内存上的

三种方式均支持阻塞/非阻塞，灵活性较大

通过映射操作，可以将设备全局存储器区域映射到主机可以访问的地址空间，因此OpenCL 2.0实现了**共享虚拟存储器**（Shared Virtual Memory，**SVM**）机制。OpenCL定义了三种SVM：

* **粗粒度SVM**：直接按照存储器区域映射的SVM
* **细粒度SVM**：按字节映射的SVM
* **细粒度系统SVM**：任意存储器区域里按字节独立映射的SVM

其中粗粒度SVM是OpenCL的核心规范，后两种细粒度实现是底层提供商的可选项

### 平台模型

从平台模型方面看，每个OpenCL设备会含有一个或多个**计算单元**（Compute Units），每个CU又由一个或多个**处理单元**（Processing Elements）构成，PU是设备上执行数据计算的最小单元，编程模型的`work-item`就被映射到这些PU上

在OpenCL设备平台上，运行kernel时会创建一个**整数索引空间**，这是一个N维值的网络，称为NDRange，其中N为1、2、3，索引空间的每个坐标标识了对应的work-item，从而实现细粒度分解并行

> 之所以这么做的原因主要是为了平衡实际存储器和存储器模型与计算设备的对应关系，在后面的文章中再讲述

## 并行化



### 并行化计算模型







### 并行化硬件





## 异构计算














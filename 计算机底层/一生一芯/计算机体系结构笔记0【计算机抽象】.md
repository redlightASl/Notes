> 本博文参考胡伟武《计算机体系结构基础》、P-H《计算机组成与设计-硬件/软件接口（RISC-V版）》《计算机体系结构：量化研究方法》和Bryant《深入理解计算机系统》写作。笔者系初学，如有错误敬请指正。

# 计算机体系结构笔记0【计算机抽象】

本篇博文主要基于“抽象”这个线索，探讨计算机分类、结构、性能的*量化*描述，基本对应于上面所述参考教材的第一章、第二章

计算机是一套非常复杂的系统，它的理论基础来源于香农的信息论和*图灵机*：是对人类进行计算行为的模仿。从中衍生出的*冯诺依曼架构*和*哈佛架构*解决了这些数学模型不能在电子电路中运行的问题，电子计算机从抽象的数学概念变成可以分模块构建的设备。数字电路和数学理论构成了计算机的最底层——计算架构，其他计算机设备都是在其基础上发展而来。但很显然程序员不用考虑CPU内部的门电路是怎么执行一条if语句，芯片IP设计者也不需要考虑上层的Windows怎么读写自己的外设，这都是因为计算机采用了**抽象**的设计思想。

抽象是计算机结构中的最核心思想，它与*分层*思想、*封装*思想一般同时使用。随着计算机底层资源（单芯片上的晶体管数目）随**摩尔定律**发生指数级性能提升，成本快速下降，上层的硬件电路和软件程序复杂度也会显著增长，对应的设计时间也会显著延长。为了上层开发者能够保持高效开发，抽象、分层、封装、虚拟化乃至软件工程的理论开始出现。

抽象就是将底层的实现细节隐藏封装起来，高层只需要使用它提供的一套简单模型和操作接口就可以实现需要的功能。这里还需要提到**分层**概念，计算机是像金字塔一样的层叠结构，基础的板级硬件电路和芯片上集成电路构成了最底层，还有通过各种协议和接口与主体连接的外部设备；更上一层是片上的核心部件CPU及与他紧耦合的电路设备；电路和软件通过ISA指令集体系结构连接，对应的机器码或者说汇编语言构成了这一层，它们是软件和硬件的桥梁；操作系统作为CPU的“驱动程序”，与转换高级语言和汇编语言的编译器处在更高一层；在其之上则是应用软件和更能被软件开发者理解的高级语言。

在计算机体系结构中，抽象无处不在，而抽象带来的效果就是**虚拟化**。虚拟化直白说就是“上层看上去是这样，底层的执行实际上是那样”，利用虚拟化就可以将底层的功能和上层的功能分离开。这个理论相对复杂，这里暂且放下，我们先回到更实际的东西上。

## 计算机性能的量化描述

在深入理解计算机系统之前，先考虑一个抽象的计算系统——图灵机。图灵机的概念就不提了，我们可以发现它存在一个理论上的计算瓶颈：完全顺序计算。一台图灵机的性能取决于其移动纸带的速度+向纸带上记录的速度+执行计算的速度；反映到冯氏计算机乃至一个实际的CPU（带有存储器和IO），计算速度就被这样加载一个数据-处理一个数据-输出一个数据的计算模式限制了

一个小学生做10道计算题，他会算得很慢，需要10分钟做完。我们要想让计算的速度提升，有下面几种方法：

* 换一个高中生来做计算题，可能只要5分钟就能做完
* 让2个小学生同时做题，一人负责5道，只需要5分钟就能做完
* 让小学生提前学习做题方法，就能用7分钟做完

也就对应了：提升硬件效率、并行计算、改善算法三种加快计算的途径

### 计算机分类

要探讨计算机的性能概念，我们首先得按照功能和工作领域把计算机进行分类——计算机如此深入我们的生活，每一类计算机都有自己的性能指标

**响应时间**又称为**执行时间**，描述一个程序执行完毕的时间；**吞吐量（吞吐率）**又称为**带宽**，描述了在单位时间内能够完成的任务数；**实时性**表示在一个突发情况发生后，设备能够在多长时间之内完成需要的处理动作；**可靠性**又被称为**鲁棒性**，表示机器能够稳定运行的时间和应对设备工作条件变化的能力

* 嵌入式设备

    身边随处可见的计算机，一般以MCU、DSP或者小SoC为体现，通常不需要太高的运算速度，但是需要**低功耗**、高**实时性**、高**安全性**和极**低故障率**。通常使用简单的算法和冗余设计来保障鲁棒性强。通过高度集成来降低工作电压，从而保证低功耗。CPU主频不会很高，但也不能太低，一般都在10M~1GHz之间，这样能保证设备具有很高的实时性，功耗也不太高。存储器一般和CPU封装在一起，还会加入安全协议来保证程序不被轻易读取乃至破解。设备启动速度非常快，这样能保证一旦通电就可以开始工作

* 个人移动设备

    随着智能手机普及，个人移动设备中的SoC逐渐成为主流的计算机形态。这种设备会将CPU、存储器、IO、外设乃至传感器、射频设备、GPU、NPU、硬件加速器等等封装在一起以节省外部PCB面积。设备的CPU主频一般在1G~3GHz之间，能够承担一些基本的运算任务，某些针对高性能设计的CPU还能够处理简单的多媒体和科学计算。从这里开始设备以**响应时间**为主要目标。

* 桌面计算（PC）

    桌面计算比个人移动设备的**响应时间**要更低，从而能处理复杂的办公、多媒体、科学计算任务。传统的计算机就是指桌面计算平台，由分立的CPU、GPU、DDR、硬盘、电源等设备构成。随着摩尔定律演变和芯片集成化趋势，桌面计算机也越来越小，从早期和桌子一样大的PC逐渐演变成MATX、ITX规格的小型主板主机，占地仅有机顶盒大小

* 服务器

    数据中心里面一个一个的柜子里面放着的就是服务器，最便宜的服务器和一台PC差不多价格，但是它更强调**吞吐率**，或者说**带宽**而不是响应时间。服务器往往可以能在给定时间内完成更多任务，其响应时间不一定比同价位的PC更快。一台服务器往往会采用多CPU、大量内存的结构来应对大规模数据涌入，很多时候不会搭配有图形界面（甚至没有显示设备），只能通过串口或者以太网访问。服务器的**可靠性**也很重要，PC或者手机死机只会让人无能狂怒凿穿键盘，但服务器宕机很有可能造成网站停摆、公司被迫停工乃至大规模数据泄露

* 边缘计算中心

    随着计算机发展，小型设备的计算性能逐渐增加。出于摩尔定律放缓的态势，各大厂商都在尝试从通用计算走向专用计算。FPGA、GPGPU这些设备没有同水平的CPU那样的通用计算性能，但对某些特殊领域的计算更加擅长，因此企业会在更接近用户的位置部署边缘计算平台，使用算法特化的FPGA、GPGPU等设备来满足某些特殊需求，比如自动驾驶（特斯拉里面的GPU就是一个边缘计算中心）。这种设备的性能需求**与要执行的工作有关**——自动驾驶汽车里面的GPU要求高响应时间，CPU要求高实时性和安全性；学校机房里的设备则更需要可靠性和吞吐量

* 计算机集群/仓储式计算机

    大量服务器通过数据背板或分布式网络组成一个集群，这就是仓储式计算机。大量服务器通过虚拟化技术，将它们的计算力合并起来，再分配到一个个虚拟机中，根据虚拟化技术不同和用户需求可以完成桌面计算到数据中心计算的多种用途。但其中硬件总的来说需要比一般服务器更高的**吞吐量**和**安全性**、**可靠性**

* 超级计算机（超算）

    针对高端科学工程计算而设计的服务器，往往代表了最高的通用计算能力，其**响应时间**、**吞吐量**、**可靠性**都是顶级的

* 云计算设备

    传统服务器逐渐被云计算所取代。大量仓储式计算机构成的巨型数据中心通过计算机集群和虚拟化技术，将海量计算资源出租给公司或个人。一些大公司会将云计算作为自己的扩展业务，向其他厂家提供云端的服务器租赁业务。云计算对于**可靠性**的需求极高，因为一旦服务器出问题，会有大量相关企业受害，一般会采取多机热备、异地容灾措施，一旦有服务器掉线就要有人去进行维护

可以发现，**响应时间**、**吞吐量**、**实时性**、**可靠性**是衡量计算设备的四个主要维度，而在PC、个人移动设备和服务器这三个主流计算机形式中，完全可以以响应时间和吞吐量作为衡量计算机性能的标准。

吞吐量和响应时间不是孤立的。**一般来说降低响应时间总是能增加吞吐量；但增加吞吐量并不一定能够降低响应时间**，也就是说响应时间可以相对公平地反应计算机性能

> 服务器中常见的多核架构比起PC中的单核架构并不一定提高的响应时间，只是通过增加处理器数目来应对更多数据处理需求从而增加吞吐量

### 并行架构

并行架构是贯彻计算机发展过程中一根不变的*红线*。人们追求并行带来的更高效率，但是也被并行编程深深困扰。

计算机中常见的并行分为六种：

* **数据级并行**（DLP）：在软件程序中实现，同时对多个数据项进行操作。常见的BLAS（基本线性代数子程序库）如OpenBLAS、CuBLAS等和常见的并行编程库如OpenCL、OpenMP等都含有以DLP思想实现的API
* **任务级并行**（TLP）：在软件程序中实现，将任务以并行方式进行处理。现代桌面操作系统就是任务级并行思想的体现。常见的线程、进程、协程都是TLP的具体应用
* **指令级并行**：CPU中的超标量就是为了实现指令级并行，即在宏观上实现多条指令并行执行而不影响执行的结果
* **向量体系结构**：也可以称为矩阵体系结构，因为该体系结构主要是对1*n向量进行运算所以就称为向量体系结构。向量指令是将单条指令并行应用于一个数据级，为软件提供DLP的具体实现。具体来说Intel的AVX512指令集、Nvidia的CUDA都是利用了类似思想。比如对加法指令，标量加法就是用C=A+B执行，向量加法则是用C[]=A[]+B[]执行矩阵加法，令A和B矩阵中的每个对应值相加
* **线程级并行**：任务级并行移到硬件上就变成了线程级并行。多核CPU就是一个典型实现，两个核心可以真正地同时运行两个乃至多个任务
* **请求级并行**：针对高性能服务器衍生出的并行思想，旨在将并行的线程解耦来解决线程级并行难以处理多个请求同时到来的问题

前两种并行是特指软件层面实现的“并行”，实际上是给用户一种并行的感觉，硬件执行程序时还有可能是串行的。后面四种并行是硬件层面实现的“真并行”。这六种并行常常因为指代不明而令人困惑，因此20世纪60年代，Michael Flynn提出了一种简单而经典的分类方式，如下所示

1. 单指令流、单数据流（**SISD**）：单处理器。程序员将其视为一个标准的顺序计算机，但是可以利用指令级并行的超标量和推理执行策略
2. 单指令流、多数据流（**SIMD**）：同一条指令被多个处理器针对不同的数据流同时执行。SIMD计算机可以将多个数据并行执行相同的操作实现数据级并行。每个处理器有其自己的数据存储器，但只有一个指令存储器和控制器，用来取指令和分派指令。现代CPU通常都有向量扩展指令集，就是用了SIMD并行
3. 多指令流、单数据流（**MISD**）：到目前为止没有这种类型的商用多处理器面世，只是这里列出来使这种简单的分类更加完整
4. 多指令流、多数据流（**MIMD**）：每个处理器取其自己的指令并在自己的数据上执行，从而实现硬件任务级并行。通常MIMD比SIMD更加灵活，因此适用性更强，但一般比SIMD价格昂贵。现代多核处理器采用的紧耦合MIMD体系结构和现代服务器常用的分布式技术都应用了MIMD并行

### CPU执行时间

> 可能有人会疑惑我们在设计CPU之前为什么要费尽心思量化描述计算机的性能。这实际上是在为新的CPU指定一个标准——我们要先有性能规范，才能规划新设计的CPU的性能范围，从而检验这个CPU的设计是否合理、效率是否足够高
>
> 本部分所讲的内容都偏向理论分析，下一节会引入一套评估计算机性能的标准软件程序

**在量化描述计算机性能的过程中，时间是唯一完整而可靠的指标**。对于一台计算机，我们可以用下面公式衡量他的性能（响应时间）
$$
性能=\frac{1}{执行时间}
$$
如果要比较两台计算机的性能，只需要比较它们执行相同程序的时间，其中较短的那个性能就更好。一般可以使用执行速度的倍数n来进行性能比较
$$
n=\frac{性能X}{性能Y}=\frac{执行时间Y}{执行时间X}
$$
根据这个指标，**同样计算任务负载下完成任务所需时间最少的计算机是最快的**。但是由于计算机经常被共享使用——多用户、多程序的条件下，计算机性能优化也可以通过提高吞吐量来实现，为了区分由提高吞吐量带来的性能提升和由减少单个任务的执行时间带来的性能提升，我们引入了CPU执行时间的概念

**CPU执行时间**：简称CPU时间，表示执行一个程序过程中只在CPU上花费的时间，不包括等待IO或运行其他程序中花费的时间。可以细分为**用户CPU时间**和**系统CPU时间**，用户事件表示程序本身花费的CPU时间，系统时间表示为了执行程序而花费在操作系统上的时间，二者通常难以区分，因为这与操作系统本身的功能结构有关。对应于这两个时间，可以用**系统性能**来描述空载下系统的响应时间，系统性能反映了操作系统的效率；用**CPU性能**描述用户CPU时间对应的性能表现，反映了单纯执行任务时CPU的性能

不同的应用需要关注计算机系统性能的不同方面，在服务器上的应用往往存在IO瓶颈而不是CPU瓶颈，IO速率决定了其性能下限；而嵌入式设备的应用往往不需要操作系统运行，其主要看重CPU性能，响应时间是其决定性考量因素。

### 处理器性能公式

想要更精确地推导出处理器性能度量公式，我们要更深入一点。我们都知道数字电路基于系统时钟完成工作，时钟确定了每个寄存器状态变化的时刻，因此我们建立**时钟周期**的概念：控制CPU所有寄存器的时钟信号的周期。我们可以用时间T来描述时钟周期，也可以采用其倒数即频率f来描述时钟周期。

我们先不考虑操作系统，以CPU执行时间为CPU性能的基本指标，可以得到公式
$$
CPU执行时间=执行对应程序所需CPU时钟周期数 \times 时钟周期
$$
也可以用时钟频率表示为
$$
CPU执行时间=\frac{执行对应程序所需CPU时钟周期数}{时钟频率}
$$
为了改进计算机性能，我们往往可以使用两种方式：

* **减少程序所用CPU时钟周期数**
* **提高CPU时钟频率**

需要注意：这里的两种方法仅限于在硬件层面实现，不包含对软件算法的优化。这也导致二者可能会是冲突的——有些优化技术在减少程序所用时钟周期数同时还会降低CPU主频，另一些技术在提高CPU主频的同时会增加程序执行需要的时钟周期数。多数时候我们应当在二者之间权衡

我们继续深入一点，上面*执行对应程序所需CPU时钟周期数*是一单个程序的层次描述CPU工作，而程序是由大量单条语句组成的，因此我们要先将这个程序分解成一条条指令，总的程序执行时间会等于每一条指令的执行时间之和
$$
执行对应程序所需CPU时钟周期数=\sum_{i=1}^{N} (单条指令所需时钟周期数_i \times 指令_i)
$$
每条指令的执行时间总是不一样的，但为了某些原因（可能是译码方便、电路简单、规划简便等等），CPU总是被设计成让所有指令都具有尽可能一致的执行时间。于是我们可以用平均数乘条目数的方法来描述总的CPU周期数，引入**指令平均时钟周期数**，表示执行每条指令所需的时钟周期平均数，用其首字母CPI（Clock-cycle Per Instruction）表示。可以获得下面的公式
$$
执行对应程序所需CPU时钟周期数=指令平均时间周期数\times 程序的指令数
$$
CPI相比CPU执行时间的优势不仅在于缩写简单（？），而且**能够衡量相同指令集在不同实现下的性能差异**。在指令集不变的情况下，一个软件程序所需要的指令数是不会变的（*这里的程序指的是汇编程序！当然高级语言用相同的编译器开相同的编译条件也可以编译出相同的指令*）

现在，我们将前面几个公式结合一下，就可以得到**经典CPU性能公式**
$$
CPU执行时间=\frac{指令数 \times 指令平均时间周期数}{时钟频率}
$$
即
$$
CPU执行时间=\frac{N \times CPI}{f_{clk}}
$$

根据这个公式，我们将计算机的性能这个概念抽象成了数学模型，从而能够*量化研究*其基本性质。在计算机科学（或者说计算机底层理论）中，**量化研究方法**是非常普遍的：基于数学提炼出计算机结构中的关键性质，这是设计计算机具体结构的关键参考

### 晶体管的功耗

芯片产业随着摩尔定律发展，决定了越大的处理器具有越大的功耗开销，因此早期集成电路产业推崇“大即是好”，不断扩张面积和功耗。但随着散热成本提高和移动计算设备兴起，处理器的功耗开始变得重要起来，这一转变发生在典中典的奔腾4时期。奔腾四有着超级深的流水线、超级高的频率、超级大的面积和功耗，还有超级低的性能，这导致了同期Intel被AMD压着打。

CMOS晶体管的能耗最大的来源是**动态能耗**，即开关过程中产生的能耗。单个晶体管翻转一次需要的能耗为
$$
能耗=\frac{1}{2} K \times 负载电容 \times 电压^2
$$
而每个晶体管的功耗是一次翻转需要能耗和开关频率的乘积
$$
功耗=\frac{1}{2} K \times 负载电容 \times 电压^2 \times 开关频率
$$
其中开关频率是时钟频率的函数，负载电容由晶体管的扇出（输出的晶体管数量）和工艺水平决定

虽然动态功耗占据主要部分，但由于漏电流不可避免，静态功耗也会占据约40%的总功耗。一般通过下面的几种方式来降低功耗

* **降低电压**：可以显著降低动态功耗，但电压下降过多会让晶体管漏电流变大，这会导致静态功耗变大
* **提高工艺水平**：曾经主流的降低功耗方法，每一代新工艺总能让能耗降低一大截，但随着摩尔定律放缓，该方法前途未卜
* **降低开关频率**：对于处理器来说，降低频率并不一定是坏事，有些移动端和嵌入式处理器本就用不到太高的性能，将频率降低一些来提供更长的续航是很划算的。现在也有Dynamic Boost等技术可以动态调节开关频率，以适应各种环境需要。

## 如何提升性能

下面来到现实问题：怎么提升计算机的性能？

先从性能公式入手，我们可以通过运行基准测试程序来测量CPU的执行时间，基准频率可以根据实际的CPU硬件设计得到，CPI和指令数却不是那么好测量，不过只要知道二者之一我们就能推断出另一个。指令数相对好计算——我们只要知道指令集就可以在不依赖于计算机具体实现的条件下基于硬件计数器完成指令数的测算；但CPI高度依赖于计算机的实现细节，对于不同程序、不同CPU实现都是不同的，我们把这个性质称为“CPI根据指令分布不同而变化”

> **指令分布**是在一个或多个程序中，对指令的动态使用频度的评价指标

尽管如此，我们还是能够保证算法、编程语言、编译器优化、ISA一定的情况下，依靠具体硬件结构推测出CPI，并进而确定CPU的性能。

把这个推断的过程反过来就得到了提升计算机性能的方法。我们将其分成下面几个要点：

* 软件层面：提高算法效率、提高编译器优化等
* 硬件层面：提高基准频率、提高硬件并行度、优化存储速率等
* ISA层面：压缩指令集提高指令密度、扩展单条指令能完成的任务

更概括来说，可以得到下面几个要点

### 并行架构

在上面的“并行架构”中介绍过四种典型并行架构。对于CPU来说，最有效提升性能的方法就是或用这些并行架构。抛开软件不谈，CPU充分利用指令级并行乃至线程级并行是非常重要的。因此人们提出了流水线、超标量、动态加载、推理执行、超线程、多核处理器等架构。在更基础的数字电路中，组相联、超前进位等数据级并行技术也很常见

### 局部性原理

**局部性原理**：程序常常重复它们最近使用过的数据和指令

这是一条经验公式，说明我们可以根据一个程序最近访问的指令和数据比较准确地预测它近期会使用哪些内容。局部性原理的适用范围包括**时间局部性**和**空间局部性**，前者是指最近访问过的内容可能会在短期内被再次访问，后者则是指地址相互临近的项目很可能会在短时间内被用到。CPU中用于分支预测的BTB分支目标缓存、PHT都是局部性原理的应用

### 优化常见情形

最重要，最普遍的计算机设计原则就是重点关注常见情形：**常见情形优于非常见情形**

对于资源分配、性能、功耗、可靠性，这点都是有效的，处理器中指令提取与译码器的使用比乘法器多，因此要优先对它们进行优化。常见情形经常比非常见情形更简单，完成速度也更快，在计算中数据溢出是很少见的，因此强化非溢出计算结果的处理比强化溢出计算结果的处理对计算机性能的提升更高

这个原则在使用时需要注意：首先判断常见情形是什么，然后再根据**Amdahl定律**决定是否对其优化

### Amdahl定律

**Amdahl定律描述了通过改进计算机某一部分能得到的性能增益**

我们定义
$$
加速比=\frac{整个任务在未采用升级时的执行时间}{整个任务采用升级后的执行时间}
$$
加速比能够反映外界条件不变的情况下，经过升级的计算机上运行一个任务可以加快多少速度

Amdahl定律指出：加速比取决于两个关键因素

* **升级比例**，即原计算机计算时间中可升级部分所占的比例
* **升级加速比**，也就是整个程序通过升级单个部分而获得的改进

**Amdahl定律的公式表示如下：**
$$
加速比=\frac{原执行时间}{新执行时间}=\frac{1}{(1-升级比例) + \frac{升级比例}{升级加速比}}
$$
这个表达式和经济学中的边际收益递减规律类似：如果仅改善系统的一部分性能而不顾其他部分，那么在增加改进加速比时，所获得的系统整体加速比增量会逐渐减小。**若某个升级仅对一项任务的一部分适用，那么该任务的总加速比不会超过一个数值**，即1减去未升级部分所占比例，再取其倒数，我们也可以用“木桶效应”来描述它——这便是Amdahl定律的隐含内容

> 系统设计中，我们通常要讲究统筹兼顾，不能妄想只优化单一性能就让系统整体水平得到飞跃式提高，并且要优先优化相对差的那部分子系统的性能

## SPEC

我们已经得到了经典CPU性能公式，那么该怎么量化研究公式中这些因素的值？一个最简单的办法就是使用**SPEC** CPU基准测试程序

SPEC是许多计算机销售商共同出资赞助并支持的合作组织，目的是为现代计算机系统建立基准评测程序集。目前SPEC组织推出的SPEC2006包含了12个整数基准程序集和17个浮点基准程序集，涵盖了各种计算机用户操作中的*经常性事件*

SPEC采用标准化的计分形式：将参考处理器的执行时间除以被测计算机的执行时间，得到**SPEC分值**

## 计算机领域的关键思想

* 摩尔定律
* 抽象
* 加速经常性事件
* 并行化
* 流水线
* 预测
* 存储层次
* 冗余

这是《计算机组成与设计-硬件/软件接口》一书中反复强调的几个关键思想，贯穿了计算机底层的方方面面，还影响到了上层软件工程的设计思想

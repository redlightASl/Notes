# 三维重建1【计算摄影基础】

本系列博文旨在构建从底层CMOS传感器硬件到顶层算法的三维重建全栈知识架构，但重点在于图像信号处理和三维重建算法。希望能记录自己在学习三维重建过程中学到的知识，并为各位读者指明学习方向，抛砖引玉。

主要框架参考[Yvon Shong](https://www.zhihu.com/people/yvonshong/posts)和[Wang Hawk](https://www.zhihu.com/people/hawk.wang/columns)的博文撰写，其他参考资料包括《智能传感技术课程PPT》、冈萨雷斯《数字图像处理》

让我们从基础知识开始，首先来看看现代数码相机是怎样获得一张照片的

## 计算摄影概览

现在一个相机可以大致分成三个部分：**镜头**、**传感器**、**控制电路**。物体上传来的入射光穿越相机镜头进入传感器，传感器集成了读出电路，可以把光信号转化的电信号发送到主控SoC，最后交给相机内的图像处理系统转换成最终的RGB图像，并存储起来或者显示在LCD上

![image-20240117210209637](三维重建1【计算摄影基础】.assets/image-20240117210209637.png)

**镜头**是一个独立的大部件，它可以和相机机身分开，镜头处理光学部分，机身处理光电混合及电学部分。因此这里先抛开镜头，来看看电信号是怎样被采集、传输、转换的

照相机的基本工作原理是**小孔成像**，经过放缩的物体的实像被投射到光电传感器平面，光信号首先进入下图绿色框表示的**CCD**或**CMOS**光电传感器，转换成电信号，通过内部集成的模拟前端采集后以**RAW**格式输出。RAW图像只给出了每个像素点的亮度信息，需要使用下图红色框表示的**ISP**（Image Signal Process，数字信号处理）算法来进行处理才能得到人眼可辨别的图像。

![image-20240117204631579](三维重建1【计算摄影基础】.assets/image-20240117204631579.png)

第一步是进行**白平衡**（WB）处理，它的作用是让人眼感知为白色的物体在最终的成像中也为白色，即对图像的颜色进行矫正。接下来是**去马赛克**（Demosaicing），这一步的目的是将RAW格式图像（每个像素都是单通道，只包含亮度信息，但其位置排布包含了不同的颜色信息）恢复成带有颜色信息的RGB/YUV格式图像（每个像素都是三通道，包含RGB或YUV三种信息，也是我们日常生活所能看到的带颜色的图片）。然后对图像进行**去噪**（denoising）。未经处理的图像通常会包含很多噪声，表现为图片上的黑点，这一步就是要让图像变得“平滑好看”。下一步是对图像进行**色调重建**（Tone reproduction），这步也叫做**Gamma矫正**。传感器对入射光强的响应是线性函数，人眼对输入光的响应也是线性的；但人的感觉却是非线性，且对暗光会更加敏感，可以用Gamma函数来近似描述人对输入光强的感觉；而显示设备相对输入图像的明暗信息响应也是非线性的，且对亮光信息更加敏感。在三者的共同作用下，就需要在相机内部对图像做一次Gamma校正来让图片看上去更加“舒服”。

经过上面的步骤，我们就能得到RGB/YUV格式的图像了。如果设备是照相机，完成这些处理以后就可以把照片压缩成JPEG格式或者以其他高级的媒体压缩格式保存到存储器中，或者直接将图像显示在LCD上。

以上所有内容就是照相机或者说摄像头的全部工作流程。其中涉及到光学、光电传感器、数模混合电路、高速数字电路、图像处理和人工智能算法等复杂的知识，本系列博文要着重强调的还是其中涉及电路和算法的部分，本篇的剩下几章主要是对光学、传感器部分进行介绍，并对ISP所需要做的工作做概括性描述。

## 摄影光学基础

看完了照相机的工作过程，我们从光学部分开始，看看光线进入照相机的第一步

### 小孔成像

摄像头或者说摄像机的基本原理就是小孔成像，用到的各种光学镜头也都是为了弥补小孔成像的不足。

> 笔者觉得自己的讲述实在不如原文来得好，这里直接放上来自https://zhuanlan.zhihu.com/p/95059112的文段

![image-20240117210921785](三维重建1【计算摄影基础】.assets/image-20240117210921785.png)

![image-20240117211017671](三维重建1【计算摄影基础】.assets/image-20240117211017671.png)

![image-20240117211131360](三维重建1【计算摄影基础】.assets/image-20240117211131360.png)

![image-20240117211144199](三维重建1【计算摄影基础】.assets/image-20240117211144199.png)

![image-20240117211203483](三维重建1【计算摄影基础】.assets/image-20240117211203483.png)

![image-20240117211214928](三维重建1【计算摄影基础】.assets/image-20240117211214928.png)

![image-20240117211233417](三维重建1【计算摄影基础】.assets/image-20240117211233417.png)

![image-20240117211245756](三维重建1【计算摄影基础】.assets/image-20240117211245756.png)

![image-20240117211255223](三维重建1【计算摄影基础】.assets/image-20240117211255223.png)

![image-20240117211322557](三维重建1【计算摄影基础】.assets/image-20240117211322557.png)

> 好的，原文就放到这里。原文后面还有大量关于小孔成像一般性原理的介绍，笔者决定讲其修改后放到后面的部分介绍

### 镜头

理想的小孔可以很好地成像，但通过上面的介绍我们已经知道，具有一定尺寸的真实世界的小孔会产生产生光的衍射（小孔太小）或同一个像点的光会来自于多个物点（小孔太大），二者都会导致成像模糊，让图像噪声很大。因此只有在两者之间某个平衡的尺寸，才能让成像既比较清晰，又具有较低的噪声。

我们可以用**薄透镜模型**来改进小孔模型。薄透镜模型是一种简化的镜头模型，它是现实中设计非常良好的镜头组的模拟。它有两个关键的假设：

* **穿过光心的光线会直线传输**
* **平行光穿过镜头后，会汇聚到焦平面上的一点上**

如果一个物点发出了一束光穿过薄透镜，那么

* 穿过光心的光线会直线传播
* 其他光线会汇聚到焦平面的一点（焦点）

如果是从一个平行于镜头的平面上发出的光线穿过薄透镜，那么

* 所有光线汇聚在同一个平面

![image-20240117213856934](三维重建1【计算摄影基础】.assets/image-20240117213856934.png)

![image-20240117213912048](三维重建1【计算摄影基础】.assets/image-20240117213912048.png)

于是有薄透镜成像公式
$$
m=\frac{f}{D'-f}
$$
且
$$
\frac{1}{D'}+\frac{1}{D}=\frac{1}{f}
$$
其中m是*放大倍数*

> 特别地，当f=D‘时，又m=D=∞，这意味着无穷远处的物体通过透镜后会在焦点处成像，即**平行光通过凸透镜汇聚在焦点**；而当f=D/2时，有m=1，此时成的像和物体大小一致
>
> ![image-20240119114637678](三维重建1【计算摄影基础】.assets/image-20240119114637678.png)
>
> ![image-20240119114647184](三维重建1【计算摄影基础】.assets/image-20240119114647184.png)

上面讨论的是一维视角下的情形，而实际上相机会把三维物体映射为二维影像，我们这里考虑三维物体的一个二维面。从物体上一点发出的光线通过透镜后在像平面上会变成一个二维投影，镜头是圆形的，那么这个投影就是圆形的——这才是真实世界发生的情况，如下图所示

![image-20240119122900823](三维重建1【计算摄影基础】.assets/image-20240119122900823.png)

我们通常称这个投影为**弥散圆**（Circle of  Confusion）：**当恰好对焦时，弥散圆的直径为0，我们才能看到一个成像点**，这和一维情形一致；但当像平面不动，物点逐渐偏离可以恰好对焦的平面时，我们就会观察到像点逐渐变成了一个圆（也不一定是圆形，这和透镜的形状有关，获得的像总是和透镜外形一致的投影）。由于人眼感知能力有限，当弥散圆直径还没有超过某个阈值时，我们还认为投影是一个点，即成像还是清晰的，只有超过这个阈值时，成像才会变得模糊，这个阈值被称为最大**允许弥散圆**（Permissible Circle of Confusion），*其直径用δ表示*

因此在一定的像距下，要让一个物体清晰成像，物距必须处在一定范围内，这个范围称为**景深**；如果物距不在景深区间内，成像就会模糊。景深是相对的，不是绝对的，且与弥散圆直径的取值大小有着直接关系

![image-20240119123543145](三维重建1【计算摄影基础】.assets/image-20240119123543145.png)

景深随镜头的焦距、*光圈*、物距的不同而变化。景深和焦距的平方成反比：焦距越大，景深越小。景深和光圈值成线性关系，光圈值越小，景深越小。景深和物距成二次方关系，物距越远，景深越大

> *光圈*的含义会在后面曝光三要素一节介绍

我们可以用下列公式描述**前景深**
$$
\Delta L_1 =\frac{F\delta L^2}{f^2 +F\delta L}
$$
**后景深**
$$
\Delta L_2 =\frac{F\delta L^2}{f^2 -F\delta L}
$$
**景深**
$$
\Delta L=\Delta L_1 +\Delta L_2 =\frac{F\delta L^2}{f^2 +F\delta L} +\frac{F\delta L^2}{f^2 -F\delta L} =\frac{2f^2 \delta L^2}{\frac{f^4}{F} - \delta^2 L^2 F}
$$
对其进行近似可以得到
$$
\Delta L \approx \frac{2 F \delta L^2}{f^2}
$$
其中F是光圈值（镜头的焦距除以光圈口径），δ表示最大弥散圆直径，L表示物距，f表示焦距

> 存在一个被称为“超焦距”的特殊情况
>
> 当镜头对焦在无穷远时，景深前界（离镜头最近的一侧景深边界值）到镜头的距离称为**超焦距**。从超焦距点到相机一半的距离开始到无穷远都是清晰的
>
> 此时景深前界
> $$
> L-\Delta L_1 =\frac{d}{2}
> $$

现在我们可以用薄透镜模型来替代小孔模型了，它是能够在现实世界实现的——但模型终究是模型，两个基础假设就要求*透镜没有厚度*，事实上是不可能的。因此为了让真实的透镜像薄透镜一样，一般是需要把多个镜头组合到一起，互相补充，从而形成一个透镜组。我们通过不同的凹凸透镜组合、距离调整，就得到不同的**镜头组**。

常见的镜头组如下图所示

![v2-4d159888ac6bedde66529008835628c2_r](三维重建1【计算摄影基础】.assets/v2-4d159888ac6bedde66529008835628c2_r.jpg)

1. **长焦**镜头：长焦镜头的焦距比较长，要比感光器件的对角线大得多，可以把远处的景物拍得较大。

    目前常见的焦距有135mm、150mm、200mm 、250mm 、300mm、500mm、1000mm等，可分为中长焦镜头（焦距150mm以内，视场角在20°左右）、长焦镜头（焦距150mm到300mm之间，视场角在10°左右）、超长焦镜头（焦距300mm以上，视场角在8°以内）三种

    **由于焦距越大，景深越浅，长焦镜头看到的图像背景更加的模糊**

2. **标准**镜头：标准镜头的视场角从40°到55°，常见的焦距为45mm、50mm、55mm、58mm等。

3. **广角**镜头：广角镜头焦距一般大于25mm，视场角在90°以内；超广角镜头的焦距在16~25mm之间，视场角在 90°~180°之间

4. **鱼眼**镜头：当焦距小于16mm，视场角超过180°，则称为鱼眼镜头

    > 目前世界上最大的鱼眼镜头视场角可达220°，这种镜头的第一片透镜像鱼眼一样突出在外面，形状像金鱼眼睛

5. **微距**镜头：为了对距离极近的被摄物也能正确对焦而设计的镜头，其镜片被拉伸得更长，从而让光学中心尽可能远离感光元件。微距镜头的外形细长，像一根笔杆一样

6. **移轴**镜头：能通过改变镜头组相对于成像平面的光轴，来改变观察到景物的透视关系的镜头

7. **折返**镜头：用来实现非常超长焦距的镜头。光线入射后会经过一次或者几次反射再到达成像元件从而减小镜筒长度

8. **变焦**镜头：可以动态改变焦距的镜头，现在大多数单反相机的镜头都是变焦镜头。只不过变焦镜头只能在有限的范围内变焦，所以在设计时还需要单独确定镜头属于长焦、标准还是广角。变焦镜头比定焦镜头外观上多了一个变焦环

> 补充一下**视场角**的概念：视场角是从像平面看所能观察到景物的范围。对同一个镜头，当物体离镜头更近时就要增大像距才能对焦，此时视场角会减小；如果是变焦镜头，扩大焦距时，需要增大像距才能对焦，此时视场角也会减小
>
> *不同焦距的镜头视场角是不一样的*，这就是用焦距和视场角来区分镜头类型的原因

尽管能够通过镜头组来得到理想薄透镜的效果，但还是存在一些成像误差

* **色差**

    由于**镜头对不同波长的光线具有不同的折射率**，就会产生色差。色差主要有两种：一种叫**纵向色差**，这是由于光线穿过透镜后无法对焦到同一平面上导致的

    > 一般的镜头对短波光折射率大，长波光折射率小

    ![image-20240119125724892](三维重建1【计算摄影基础】.assets/image-20240119125724892.png)

    将两片不同材质、不同折射率的镜头组合到一起组成对称结构，能一定程度减弱这种现象，但无法完全消除

    ![image-20240119125744450](三维重建1【计算摄影基础】.assets/image-20240119125744450.png)

    还一种是**横向色差**（也叫**倍率色差**）。透镜的放大倍数也随光的波长而变化，不同波长的光也可能会聚焦在像平面的不同点，从而在它们之间出现彩虹一样的色带（尤其是高对比对区域）。倍率色差是折射率与视场的函数，可以通过移动孔径位置或者透镜间距的方式校正，通常使用折射率不同的透镜组成对称结构来进行修正，但同样只能减弱而无法杜绝

* **球差**

    薄透镜模型要求平行于光轴的所有光线穿过镜头后就汇聚到焦点，这其实是要求镜头的剖面是一个双曲线。但实际镜头并非双曲面，大多数是球面的，因此光线穿过后并非对焦到同一点，这就会导致画面的模糊

* **场曲**

    ![image-20240119163746042](三维重建1【计算摄影基础】.assets/image-20240119163746042.png)

    在透镜组中，常常会发生成像平面变成一个以主轴为对称的弯曲表面的情况（像平面为一曲面），这时候就会出现场曲误差。这会导致当调焦至画面中央处的影像清晰时，画面四周的影像模糊；而当调焦至画面四周处的影像清晰时，画面中央处的影像又开始模糊。

    场曲与孔径大小无关，所以缩小光圈无法去除，但可以在单个透镜前面再加一个透镜，二者方向对称，从而有效减小场曲

* **慧差**和**像散**

    ![image-20240119162341802](三维重建1【计算摄影基础】.assets/image-20240119162341802.png)

    受到工艺限制，透镜不同位置的折射能力总会存在差异。因此即使理想的平行光入射后，也无法聚焦到焦点，在垂轴方向也不与主光线相交，即相对主光轴失去对称性。因此会导致光线在理想像平面处不能成清晰的点，而是映射为一个彗星状的不对称光斑。这种情况被称为*慧差*

    慧差是光学孔径和视场的函数，与透镜形状息息相关，一般可以通过组合使用对称结构的正负透镜来矫正

    ![image-20240119163022624](三维重建1【计算摄影基础】.assets/image-20240119163022624.png)

    进一步地，在二维情况下，物体上某点发出的光经折射后可能会出现水平和垂直方向的焦点不一致的情况，这时就会成一个模糊的光斑实像。此时水平和垂直方向的焦距之差被称为*像散距离*

    像散是视场角、透镜形状、孔径位置的函数，与孔径大小无关，同样可以使用对称结构的正负透镜来校正

    场曲、慧差、像散的出现都是由于透镜工艺限制无法实现连续一致的折射率

* **畸变**

    平面物体主轴外的直线经光学系统成像后变为曲线，则此光学系统的成像误差称为畸变。畸变只影响成像的几何形状，而不影响清晰度，这与球差、场曲、慧差、像散有着根本区别。

    畸变可以分成**桶形畸变**和**枕形畸变**

    ![image-20240119164033999](三维重建1【计算摄影基础】.assets/image-20240119164033999.png)

    **桶形畸变**：图像放大倍率随距光轴的距离而减小，导致镜头外围成像的点向边缘弯曲（看起来被压缩）。

    **枕形畸变**：图像放大倍率随距光轴的距离而增加，导致镜头外围成像的点向中心弯曲（看起来被张开）。

* **波差**

    从波动光学的角度看透镜，一个物体的像就是一个复杂的艾里斑。由于像差，经透镜组形成的衍射波组成波面不再是规则的球面，从而导致入射光波和衍射光波会形成一个光程差，这被称为波像差，简称波差

    ![image-20240119164408708](三维重建1【计算摄影基础】.assets/image-20240119164408708.png)

### 光的度量

**光通量Φ**：单位时间内由光源所发出或由被照射物体所吸收的光能，单位是流明lm

亮度L：光源在给定方向上单位面积单位立体角内的光通量，单位坎德拉每平方米cd/m^2

**光强I**：光源在给定方向上单位立体角内的光通量，单位坎德拉cd。亮度对发光面积分就可以得到光强

**照度E**：每单位面积吸收可见光的光通量，单位勒克斯lx。可以用入射光通量除以被照射物体的受光面积得到（E=Φ/S）

**曝光量H**：像平面的照度与曝光时间的乘积，即照度对时间积分，H=∫Edt。曝光量的倒数**S**=1/H被称为**感光度**

很明显，当底片或者说光电传感器吸收的光强越大、时间越长即曝光量越大，得到的图片色彩就更亮

### 曝光三要素

相机传感器的三个重要参数：**光圈**、**快门**、**ISO**决定了曝光量和成像效果，它们被称为”曝光三要素“

首先来介绍**光圈**，这个概念在前文也有出现，它指的是照相机镜头中控制光线的叶片装置。如下图

![image-20240119173512735](三维重建1【计算摄影基础】.assets/image-20240119173512735.png)

光圈就相当于小孔成像中的小孔，只不过实际的光圈通常不是圆形的，而是机械结构导致的多边形。可以调整光圈大小来调节透镜的通光量，光圈中心开口的大小代表光圈的数值，用F系数表示。**光圈数值F越小，孔的开口越大，有更多光能通过镜头，因此曝光量越大**

> 通常在拍摄时所说的“开大光圈”是指把光圈的数值调小、让光孔开大

**光圈的值通常是$\sqrt{2}$的倍数**（公比为1.4的等比数列），比如f1、f1.4、f2、f2.8、f4、f5.6、f8、f11、f16、f22、f32。当光圈半径呈$\sqrt{2}$倍关系时，面积就会呈2倍关系，通光量也呈2倍关系

> 在后文中可以了解到，在现代的CMOS图像传感器中，光圈也可以通过人工设置一个增益值Gain，将所有像素值都乘一倍数来设置（光圈可以部分看成镜头成像明暗的参考量）

下面是**快门**。快门用于控制入射光照射到传感器上的时间，光圈控制H=∫Edt公式中的E，而快门就控制了dt。我们可以通过调节快门开闭时间长短和开启的空隙大小来调节曝光量

![image-20240119174651463](三维重建1【计算摄影基础】.assets/image-20240119174651463.png)

上图展示了快门关闭时的样子，它实际就是挡在底片或传感器上方的遮光帘。

快门参数基于速度标定，如1/4、1/8、1/15、1/30 、1/60、1/125等。它们之间是倍数关系，指的是几分之一秒的曝光时间，因此通过快门的曝光量也呈倍数关系

快门根据开启的方式分为**全局快门**和**卷帘快门**，上图就展示了一个典型的卷帘快门。全局快门不容易使用机械结构制造，一般来说只能通过电路让感光元件上所有的感光小单元同时感光来实现，制作工艺要求高，一般只用在CCD传感器或高端的CMOS传感器上。不过随着近年来半导体技术提升，已经有越来越多的图像传感器可以使用高速全局快门了

卷帘快门是让感光单元从上至下逐行曝光，开启时就如下图所示，CMOS图像传感器之间暴露在外界，接受曝光

![image-20240119174830016](三维重建1【计算摄影基础】.assets/image-20240119174830016.png)

卷帘快门在慢速快门和高速快门下，工作方式是不一样的：快门**慢速工作**状态下，机械快门有前后两个帘幕，后帘幕靠近图像传感器，曝光时它从下至上先打开，接着靠近镜头一侧的前帘幕从上至下再打开，此时光线从上至下照进传感器。再接着后帘幕从上至下关闭，光线也从上至下慢慢消失，前帘幕再关闭，结束曝光。以上步骤可以使传感器曝光非常均匀，照片不会产生太大的形变。**高速工作**状态下，不能依靠两个帘幕分别开关来工作——很耽误时间——于是用固定缝隙扫过感光元件的办法来实现等效“曝光时间”。高速的卷帘快门控制一个曝光条从上至下依次扫过感光元件，让感光元件的总曝光时间一定，但曝光区域的位置一直在变化，其中一帧如下图所示

![image-20240119184824421](三维重建1【计算摄影基础】.assets/image-20240119184824421.png)

快门速度越高，就让曝光条越窄、扫过感光元件的速度越快，这样曝光时间就越短。

过慢的快门速度会导致一张照片中从上到下反应的其实并不是同一个时刻时的景物状态，引起被称为**果冻效应**的图像失真（使用全局快门可以直接消除这个问题，再慢的快门速度也不会导致果冻效应）。当相机以较慢的速度逐行扫描图像传感器时，如果在曝光过程中被拍摄物体相对于相机发生了高速运动或快速振动，就会导致图像出现不稳定的情况，比如倾斜、摇摆不定或明暗不等

> 在拍摄高速运动的物体时，如果快门速度不够快拍摄的图像会模糊、拖尾，而横向运动的物体
>
> ![image-20240119183119155](三维重建1【计算摄影基础】.assets/image-20240119183119155.png)

卷帘快门的另一个问题就是会导致**难以实现闪光同步**以及**频闪**现象

闪光同步指相机的快门速度至少应该能保证在闪光灯工作的时间段内感光元件都接受到曝光，但当快门速度高于临界快门速度时，成像元件不能全部同时接收光线，照片的曝光严重不均匀（因为闪光灯的脉冲时间非常短，在1/200 s~1/500 s之间，亮度随时间迅速变化），这就使得照片上只有一个窄条曝光正常，其余部分因为没有接收到足够的曝光而显得很暗，只有当相机的快门速度低于这个临界速度的时候，闪光灯才能和相机快门同步，保证成像元件的各个部分同时曝光

至于频闪现象，笔者在这里推荐读者直接观看[影视飓风的科普视频](https://www.bilibili.com/video/BV1ua4y127pk/)，可以发现它和闪光同步的原理类似，都是由于照片曝光时亮度不均匀导致的问题

三要素中，除了光圈、快门，还有一个要素是成像元件的感光度，在摄影中我们用感光度的国际标准**ISO**指代。

*感光度是从老式胶卷继承而来的概念*。ISO数值越高就说明胶卷感光材料的感光能力越强，在目标曝光量一定的条件下，感光度越高，对光强、曝光时间的要求就越低——更低的光强能激发更大的曝光量，通俗一点说ISO就是衡量胶卷需要多少光线才能完成准确曝光的数值。ISO数值越大，胶卷对光线的敏感程度就越高，ISO 200胶卷所需的曝光时间是ISO 100胶卷的一半；但使用高ISO的胶卷带来的噪点也相对的大（受到暗光影响更显著）

![image-20240119192834850](三维重建1【计算摄影基础】.assets/image-20240119192834850.png)

对于现在的CCD/CMOS光电传感器，也存在感光灵敏度高低，只不过**本质上是模拟放大电路给到信号的增益**，详细的电路内容会在后文中叙述，这里只需要简单理解即可：ISO描述了最小需要多少曝光量才能使得传感器的光敏单元被激活。CMOS/CCD厂商为了方便数码相机使用者理解，一般**将传感器的感光度等效转换为传统胶卷的感光度值**，然后可以通过调节传感器的增益控制寄存器来调节ISO

CMOS传感器的感光元件比较特殊，存在*暗电流*，这会导致在没有光线照射传感器时也会出现输出，因此厂商在普通模式下设置了截止电流来防止干扰；但当环境光线暗时，CMOS感光元件的输出电平会处在较低水平，需要使用一个数控放大器按照对应ISO数值提供增益，把输出电平抬高到便于采集的区间；但同时，暗电流导致的噪声也会被放大，这反映到图像上就是随机的**噪点**

> 暗电流会在后文中介绍

虽然高ISO的传感器在噪点方面存在问题，但为了获得更高的快门速度和更明亮的画面，这也算是一种不增加成本的折中解决方法：低ISO适合营造清晰、柔和的图片，而高的ISO值却可以补偿灯光不足的环境

曝光三要素共同作用下才可以对传感器的曝光量进行控制，光圈大小控制曝光的**孔径**；快门速度控制**曝光时间**；ISO则控制曝光的**感光度**。**三者可以通过不同的组合来得到相同的曝光量**，比如同一个ISO下，光圈F8、快门速度1/30得到的曝光量，和光圈F5.6、快门速度1/60，以及光圈F11、快门速度1/15三者得到的曝光量是相同的。这样，摄影师可以根据自己的目的来选择光圈、快门速度和ISO

不过三者各自还有不同的特点：

* 光圈越大，曝光孔径越大，进光量越多，景深越小
* 快门越慢，曝光时间越长，进光量越多，越容易引起果冻效应，运动的景物更容易模糊
* ISO越高，传感器在暗光下的效果越好，但噪点越多

![image-20240119192523887](三维重建1【计算摄影基础】.assets/image-20240119192523887.png)

现在的传感器一般都会带有自动曝光功能，这就是让相机根据用户的预设，在自动曝光算法的控制下自动控制曝光三要素的值，从而让用户无需关心繁琐的调节过程，常见的预设如下图所示

![v2-ad3c5fe3d025947cab422eb1329bfc40_r](三维重建1【计算摄影基础】.assets/v2-ad3c5fe3d025947cab422eb1329bfc40_r.jpg)

### 等效焦距和等效光圈

数字影像系统中，镜头、传感器共同决定输出图像。因此即使限制了曝光三要素和镜头特性，传感器的尺寸也会影响输出的图像。

在传统的胶片影像年代，我们引入**画幅**的概念来表征相机输出图像的尺寸大小，当时的相机使用的是胶卷或玻璃*底片*，它们根据不同的尺寸标准和用途，被分为不同的画幅。到现在，我们将相机光电传感器（CCD/CMOS）的感光阵列面积大小称为画幅。由大至小有大画幅、中画幅、全画幅、APS-C画幅、APS画幅等，其中感光阵列尺寸和35mm胶卷尺寸相同的称为**全画幅**

![v2-5ee00675bec2d808e6f5ac9ffeb58cfb_r](三维重建1【计算摄影基础】.assets/v2-5ee00675bec2d808e6f5ac9ffeb58cfb_r.jpg)

如果我们在光学系统中保持其他不变，只改变机身使用不同画幅的传感器，我们就会看到原来大尺寸传感器的图片仿佛被裁切了一样。为了统一由光电系统整体的焦距、光圈、画幅带来的成像效果，我们引入了**等效焦距**和**等效光圈**，以它们作为不同画幅下焦距和光圈的成像效果表征

**等效焦距**是指由于画幅的变化，导致相机视野发生变化，如同焦距发生了变化。举个例子，如果焦距为f的相机所看到的景色，相当于全画幅相机使用2f焦距看到的更远更小的景色（也相当于在全画幅原图上进行了**裁切**），于是对于这个焦距为f的残幅相机，我们称它的*35mm等效焦距为2f*

为了计算方便，我们直接将35mm等效焦距记为实际焦距乘以焦距转换率，称为**裁切系数**。典型的相机裁切系数如下图所示

![v2-aee22989f7887142b6e36e7f304c0e46_r](三维重建1【计算摄影基础】.assets/v2-aee22989f7887142b6e36e7f304c0e46_r.png)

等效焦距的本质是**等效视场角**，只是由于视场角的函数表达比较复杂才不使用这个表征。视场角函数可以用下式描述
$$
f_{OV}=2arctan(\frac{W}{2f})
$$
![image-20240204121104841](三维重建1【计算摄影基础】.assets/image-20240204121104841.png)

通过和全画幅对比得到一个等效焦距，只需要做除法，显然更容易计算和记忆

再来说**等效光圈**。虽然理论上景深只取决于光圈口径和对焦距离，但是相同的光圈值在不同画幅的传感器上能够给出的*虚化能力*不同、信噪比不同，所以需要一个用于跨画幅对比虚化能力和信噪比的参量，这就是等效光圈。在画幅不同的情况下，我们就可以直接根据不同相机的等效光圈值来进行比较*景深*、*弱光性能*以及同结构下的*分辨率*等效果。

需要注意：和等效焦距类似，两个相机的等效光圈不同，它们的实际光圈值可能是相等的

> 弱光性能和分辨率都和信噪比相关，这在后文中有介绍

为了描述不同画幅相机下的景深，我们需要引入虚化程度的概念。画幅对景深的影响是由最大允许弥散圆引起的——同样大小的弥散圆，在大底相机上可能不算什么，但是在小底上简直就虚的一塌糊涂。因此我们将弥散圆直径相比传感器的相对大小称为**虚化程度**（虚化效果）

具有小画幅的相机在使用相同光圈值的条件下，其虚化能力比大画幅相机小，而为了让二者的虚化程度相同，小画幅相机需要将光圈孔径开大（减小光圈值）。根据这一逻辑，**等效光圈可以被看成小画幅相机和大画幅相机虚化效果（最大允许弥散圆直径）在相等时的光圈值之比**

不难推出：**等效光圈决定景深，等效焦距除以等效光圈决定远景虚化能力**

于是在上述条件下，我们可以得到四条基本定理：

* **物距固定、画幅固定、焦距固定的相机，光圈越大，光圈值越小，虚化程度越大；反之亦然**
* **画幅固定、光圈固定，物距越小，虚化程度越大；反之亦然**
* **画幅固定、光圈固定、物距固定，焦距越长，虚化程度越大；反之亦然**
* **物距固定、等效焦距固定、光圈固定，画幅越大，虚化程度越大；反之亦然**

## CMOS和CCD传感器基础

> 美国科学家威拉德·博伊尔和乔治·史密斯1969年共同发明了CCD图像传感器。二人在2009年被共同授予诺贝尔物理学奖。CCD自1970年问世以来，因其独特的性能而发展迅速，开创了数字影像时代

CCD是最早一代图像传感器结构，其造价相对高、成像效果相对好。以前高端设备会采用CCD，而中低端产品才会采用CMOS；而现在的CMOS工艺传感器已经和CCD传感器有相当的性能了，同时造价低廉，因此大多数消费级和工业摄像机已经采用了CMOS传感器（虽然在有特殊要求的高端领域还是需要使用CCD）

CCD传感器的结构和CMOS传感器大同小异，二者最主要的区别还是在于工艺，就让我们从CCD开始讲起

### CCD

电荷耦合器件（Charge Couple Device，**CCD**）是一种基于MOS的大规模集成电路光电器件，其基本结构如下图

![image-20240118213118753](三维重建1【计算摄影基础】.assets/image-20240118213118753.png)

其由大量**MOS光敏源**构成，实际上就是在单个NMOS或PMOS的基础上将沟道拉长、尺寸做宽并以矩阵形式构造密集排布的栅极Contact，每个栅极小岛由独立的Metal层引出（引出方式在后面讨论）。每个金属-氧化物-半导体接触都相互独立，但又靠的足够近以至于耗尽区边缘相邻，这样的排布会带来特殊的反型层（或者说耗尽区），如下图所示

![image-20240118213714533](三维重建1【计算摄影基础】.assets/image-20240118213714533.png)

每个耗尽区对带负电的电子而言都是一个势能很低的区域，称为**势阱**。以基于PMOS的CCD为例，半导体内的少子（在这里是电子）就会被吸引到MOS栅氧接触界面，让耗尽区带负电，这样的势阱被称为**表面势阱**。

当光照射在硅片上时（光子穿过硅结构），光电效应导致半导体硅产生光生电子空穴对，其中光生电子会被附近的势阱吸收，空穴则会被排斥出耗尽区，且**势阱内吸收的光生电子数量与入射到该势阱附近的光强成正比**。存储了电荷的势阱被称为**电荷包**

对于MOS光敏元，我们通常用**量子效率**衡量其光电转化性能，即转换的电子数量与入射的光子数量的比值。当不断改变入射光的强度时，MOS光敏元转换出的电子数量也会发生改变这就构成了**响应函数**——在正常工作状态下，它通常是一个**线性**函数。电荷包的最大容量被称为**满阱容量**。由于硅晶体声子只能与有限数量、固定波长的光子发生吸收碰撞来让电子跃迁，**满阱容量基本上是像素面积而不是体积的函数**，这正好能用来反映入射光的强度信息。不难联想到，当MOS光敏元转换出的电荷让电荷包达到满阱容量时，势阱会饱和，吸收更多光子转换出的电荷会让表面势阱发生扩散，这被称为**过曝**；而在没有光线入射或入射光非常少时，MOS光敏元不会在其下方势阱中积累电荷，但是P衬底中的电子分布是动态且随温度变化的（况且硅晶体中总会出现缺陷导致电子空穴对不均衡的情况），因此总会有部分电子进入到势阱中，就相当于入射光信号被*噪声*淹没了，我们使用**暗电流**来评估电荷产生的电流强度。因此，在光线过强或光线过暗的情况下，**势阱饱和与暗电流导致响应函数不再线性**。

> 在后面会由ISP努力修正这些问题

容易理解，这样的CCD就成矩阵地构造了大量MOS光敏源，每个光敏源就能够表示一个像素的信息。当大量MOS光敏源被排列起来，就可以采集入射光成像的信息了

> 请注意，两个有源区之间的沟道长度和器件宽度是有极限的，因此需要排列大量上述结构组成CCD。其实最简单的方法就是构造由三个或四个独立栅极构成的CCD光敏元，再将其排列起来，这样虽然会损失一些面积（光敏元之间需要隔开并构造有源区），但会让接收到的光信号质量最好

实际上除了上面所说的**光注入法**，还可以通过**电注入法**来产生二维排列信息。在下图中，在第一个栅极旁扩散N区，这就形成了一个PN结，当在N区加正向偏压时，PN结耗尽区的电子会经耗尽层边沿进入Φ1势阱，并通过表面势阱相邻的边沿向Φ2、Φ3扩散。如果施加的只是一个时间极短的正脉冲IGΔt，那么几乎不会发生扩散，Φ1处存储了IDΔt这么多的电荷

![image-20240118221303574](三维重建1【计算摄影基础】.assets/image-20240118221303574.png)

CCD的基础结构带来了一个问题：**其将光信号转换成电荷信号**而不是电压/电流信号，这导致它需要一个**读出电路**来把图像信息从MOS光敏元的一个个电荷包中输出到器件外。我们一般选用读出移位寄存器来“转移图像”，从外界看来就是让CCD输出幅度与电荷包成正比的电脉冲序列

> 这样转移电脉冲序列的控制方法非常类似步进电机，相同地，也被分成两相、三相、四相控制方式，通过向CCD的*栅极组*施加特定的序列电压脉冲，就可以逐次把电荷转移出去。这里以三相控制为例，如下图所示
>
> ![image-20240118215320133](三维重建1【计算摄影基础】.assets/image-20240118215320133.png)
>
> 经过一个时钟脉冲后，电荷从前一级转移到下一级的同号电极下
>
> ![image-20240118215451435](三维重建1【计算摄影基础】.assets/image-20240118215451435.png)
>
> 这样电荷就完成了在CCD内部的转移

移位寄存器也是由MOS工艺制造的，只需要在其表面涂敷遮光材料就可以保证不受光照干扰了

当然，不可能让CCD产生的电荷信号直接输出。如果你学习过电荷型传感器的基本原理就能明白，电荷信号是静态的，需要转换成电流或电压才能被外界获取。因此CCD的输出方式也分成**电流输出型**和**电压输出型**两种

还是以PMOS衬底的CCD为例，通过在衬底上扩散N有源区就可以形成PN结，我们可以构造下列结构

![image-20240118220120573](三维重建1【计算摄影基础】.assets/image-20240118220120573.png)

其中OG是CCD单个光敏元最外层的一个栅极，当在其上加正向偏压形成沟道，再往PN结的N区施加高电势（PN结反偏），就会在N区下形成深势阱，远比电荷包的势阱强度大，而OG下方的沟道则会直接导通，让电荷从通路形成电流从而再负载电阻RL上输出与电荷成正比的电压。

把MOS光敏源阵列、移位寄存器、输出电路制造在同一个P/N衬底上，这就是**CCD固态图像传感器**了（下面统称为CCD），其中MOS光敏源被统称感光部分，负责将入射光的空间分布转换成与光强成正比、大小不等的电荷包空间分布；移位寄存器和输出电路把电荷包依次转移出来并对外输出成电脉冲序列

### CCD的分类

根据光敏元排列不同，CCD可以分成**线型**和**面型**两种；根据结构不同，又可以分成单沟道CCD、双沟道CCD、帧转移结构CCD、行间转移结构CCD等。

**单行**线型CCD结构如下图所示，MOS光敏元直线排列，移位寄存器和光敏元间有一个**转移控制栅**。每次采集结束后，转移控制栅会让电荷包向对应的移位寄存器转移，同时以高频率在移位寄存器上施加脉冲，就可以把电荷信号串行输出了。此外还有**双行**结构的线型CCD，原理大同小异，只不过一次可以转移两行电荷数据；同理也可以构造出多行结构的CCD

线型CCD需要用逐行扫描的方式才能获得一幅二维图像，因此主要用于**光学扫描**和测试领域，在工控领域（尤其是工厂产品质量检测）也有应用，

![image-20240118221722258](三维重建1【计算摄影基础】.assets/image-20240118221722258.png)

面型CCD就会更复杂一点。

线转移的面型CCD由一个两相驱动的行扫描信号发生器控制。光敏元排布成二维阵列感光区，在信号采集完毕后按行依次输出，每行的输出方式都和线型CCD一样，只不过在行扫描信号发生器的控制下，每行的数据会被依次读出

> 比如一个5行5列的感光区，首先5个脉冲信号是第一行的所有电荷输出；第5~10个脉冲信号是第二行所有电荷输出，以此类推

其特点是有效感光面积大、转移速度快，但每行电路都比较复杂，在读出时难免会产生电荷损失，引起图像模糊

帧转移型CCD加入了一个存储器阵列，用于将每帧二维图像的输出暂存，所有曝光后，光敏元产生的信号被统一转移到存储器阵列中，这样避免了行扫描发生器的复杂设计，还提高了光敏元密度，但仍不能解决图像模糊的问题

**行间转移**CCD则是目前应用最多的结构，面阵CCD和CMOS大都采用该结构。光敏单元和垂直转移寄存器交替排列，转移控制栅可以受到统一控制也可以分开受控，每一列转移控制栅被施加高电压后，就相当于对应一列的MOS光敏元信息被保存，也就是曝光结束，这样可以形成“卷帘快门”和“全局快门”的效果。当对应列的信号采集结束后，垂直转移寄存器内会填充满光生电荷包对应信号，再被依次输出。

这个结构导致感光单元密度下降、设计复杂，但图像更加清晰

面型图像传感器主要用于获取二维的图像信息，大量用于**可见/不可见光图像采集**和处理

![image-20240118222502054](三维重建1【计算摄影基础】.assets/image-20240118222502054.png)

所有CCD电荷信号被变成电压/电流信号输出后，对应每个像素信号都不足以被外部电路按照标准电压进行变换处理，**需要在输出端口串联精密放大器进行放大处理**，尤其要保证放大器的带宽（为了保障在曝光时间间隔能够完成对图像的读出，读出电路的时钟信号频率要比曝光频率高得多）和噪声（防止图像产生色偏）都要非常小，这使得整个传感器的**功耗大**、**面积大**。此外，在制造过程中，只要出现一个像素结构的异常就会导致传感器感光单元全部报废，因此**CCD良品率始终不高**

一个经典行间转移面型CCD芯片的全局结构如下图所示，可以看到在光敏单元输出部分串接了运放

![image-20240118231200801](三维重建1【计算摄影基础】.assets/image-20240118231200801.png)

在图中左侧部分，增益调节（Gain）和ADC被统称为**模拟前端**（Analog Front End，**AFE**），在CMOS传感器中也有类似的结构，我们会在下面讨论。而Line Driver则是CCD传感器的数字部分，它负责将传感器的输出信号整理成标准电平和时序以适应统一的图像传感器协议（比如DVP、MIPI等），或者让采集到的原始数据直接以串行输出（RAW）

### CMOS

见文知意，CMOS之所以叫CMOS，就是因为它是基于CMOS（互补金属氧化物半导体）工艺制造的。其采集光信号再转变成电荷包的过程和CCD一模一样，使用的也是MOS光敏元，但在这里它可以采用被称为**光电二极管**的标准单元设计，因此非常容易加入CMOS大规模集成电路中。

![image-20240119012454520](三维重建1【计算摄影基础】.assets/image-20240119012454520.png)

光电二极管看起来很像CCD部分介绍的电注入法结构，由一个栅极和一个N有源区构成。版图上光电二极管的尺寸会做的很小，这就导致每个光敏元产生的电荷都非常微小，还会由于沟道长度和单元间距产生漏电问题，需要搭配外部供电才能很好地工作，因此CMOS的基本像素单位被称为**有源像素传感器**（Active Pixel Sensor，**APS**）。结构如下图所示

![image-20240119012522753](三维重建1【计算摄影基础】.assets/image-20240119012522753.png)

在曝光前，先对光电二极管进行**充电**：在栅极施加反向偏压，让P衬底中的多子（空穴）集中在势阱；随后进行**感光**，让晶格吸收光子产生电子和空穴进行配对，形成光电流；完成感光后，光生电荷就等于初始状态的电荷减去当前势阱内容纳的电荷，由于栅极电容不变，也就等于初始状态的电压减去当前电压，这就建立了从电荷到电压的映射关系。这时候只需要让光电二极管**放电**，把电压信号输出，让后续电路处理即可。因为是电压信号，每个APS信号输出就不能使用沟道了，必须使用*模拟开关*（实际上就是一个工作在线性区的且经过设计放大倍数接近1的CMOS三态门）才行

![image-20240119003429637](三维重建1【计算摄影基础】.assets/image-20240119003429637.png)

由于每个APS产生的电信号都比CCD的MOS光敏元产生的信号更加微小，因此**每个像素都被分配了独立的信号放大器**。这样的结构就让像素信号在输出前就被放大到了可以接受的程度（CCD受到结构限制，无法采用类似的方法来减小对精密放大器的依赖），这又让放大器的**带宽要求降低**了。同时，由于信号被放大，还有多个行列开关同步控制，CMOS图像传感器的**读出速度**要比CCD**快**很多。同时，CMOS工艺还能受到摩尔定律带来的红利，让CMOS传感器的**集成度变高**、工艺要求下降，**成本**也就**低**了不少。

![image-20240118230655322](三维重建1【计算摄影基础】.assets/image-20240118230655322.png)

然而，CMOS有一个致命的缺点：**固定噪声大**。CCD传感器使用统一的运放，“一夫当关”导致所有输出信号的噪声都是相同的，一致性很好；CMOS采用每个像素独立的运放，这就让每个像素输出信号的噪声依赖于对应运放的噪声——读者可以考虑这样一个情形：每个电阻的噪声都是1ppm，100个串联电阻带来的噪声会是多少呢？图像的信息是连续的（虽然人眼不一定能区分得出来），每个像素和相邻像素的噪声都可以视为“串联的结果”

答案是100ppm。不难想象，各个放大器不一致会带来较大的**固定模式噪声**。这便是当前CMOS还没有完全替代CCD工艺的主要原因

此外，CMOS的感光部件面积也被APS外围器件和放大电路挤占了，同时受到CMOS工艺的影响，光电二极管不会被给出太大的片上面积，**受光率**要远远**小**于CCD

> 除此之外，CMOS和CCD还有区别：
>
> * CCD制造全局快门的成本很低，只需要使用单一信号控制的转移控制栅在同时对光敏元信号读出即可；而CMOS每个光敏单元都需要使用独立的外部供电管和模拟开关管理信号读出，因此主要以卷帘快门为曝光方式，**全局快门的面积开销更大**。
> * CCD的读出电路需要外加12/18V的高电压才能驱动电荷移动，其耗电量为CMOS的8到10倍，高驱动电压还要求更精密的电源线路设计和耐压强度，这对整个器件的设计都有影响；但CMOS采用主动采集，且传输电压信号，这让其**整体功耗低得多**

在CCD作为主流的年代，生产成本高、集成度差、能耗高是其三大弊端；随着CMOS技术的兴起，固定噪声、低灵敏度则一直困扰着它。不过随着半导体技术的发展（摩尔定律），CMOS特征线宽越来越小，同样面积上能做的电路越来越多，这样留给APS的面积也就越来越大，比例越来越高

> CMOS是典型的模拟混合电路，也正是于此原由

总的来说，CMOS替代CCD已经是业界趋势，随着CMOS工艺的发展，其弊端也能够被更好地抑制。不过目前几乎所有的线型图像传感器都采用**单行或双行CCD架构**；面阵型图像传感器则采用**行间转移CMOS架构**，可见CCD还不是那么容易退出市场，CCD工艺的基础设计思想则已经贯彻在了现代CMOS器件之中。

而随着半导体技术的发展，更多新设计被应用在CCD和CMOS上，这让二者的固有问题都得到了部分缓解，下面介绍一些能够在现代图像传感器上被广泛使用的设计

### MEMS构件和堆叠设计

CCD和CMOS都依赖MOS光敏元或者说光电二极管，它在感光的状态下只能输出表示当前位置光强（灰度值）的信息，并不能输出彩色信号，怎样才能获得彩色信号呢？

目前最普遍的方法就是**拜尔阵列**（Bayer）：这是由四个APS构成的排列，每个APS只负责处理一种颜色的光，通过在上面覆盖**MEMS滤光片**可以很好地实现这一需求，四个滤光片按“红绿绿蓝”（RGGB）组成的2x2单元格排列起来。于是整个感光单元的排布就像下图

![v2-c5d403d06241d6c5fe3565b690f09cd7_r](三维重建1【计算摄影基础】.assets/v2-c5d403d06241d6c5fe3565b690f09cd7_r.jpg)

这样我们便能得到传感器对不同颜色光的响应情况。至于为什么拜尔阵列使用RGGB排布，这就要先从人眼对光线的敏感度特征说起

通常的光源都由多个波长的光组成，每种光的功率都不一样。如果画出每种波长的功率和它的波长之间的关系，我们可以得到光源的**功率谱分布（SPD）**

![image-20240119014850328](三维重建1【计算摄影基础】.assets/image-20240119014850328.png)

而传感器对不同波长的入射光线会有不同的敏感度，使用**光谱敏感度函数（SSF）**来描述。实际上我们在之前所说的MOS光敏元响应函数就是入射光SPD和传感器自身SSF乘积对入射光波长的积分
$$
R=\int_\lambda \Phi(\lambda) f(\lambda) d\lambda
$$
人眼也是如此。人眼视网膜上的视锥细胞有三种，分别对可见光的RGB波段敏感；而负责在暗光环境下感光的视柱细胞对绿色最为敏感。**拜尔阵列是对人眼感光的模拟，因此它选用了RGGB排布**。实际上色彩空间也可以由CYYM或RYYB来描述，这些颜色在光谱上覆盖范围更广，允许更多光线通过传感器，但处理起来相对复杂，所以没有大面积普及。

![image-20240119015455164](三维重建1【计算摄影基础】.assets/image-20240119015455164.png)

> 其实每个型号的CMOS传感器的SSF都可能不同，这是因为厂商都在追求“看得最舒服的颜色”而进行调色

这样的传感器已经能够用不同颜色的亮度值（灰度值）来表述三原色RGB了，但很显然原始图像（所谓的RAW格式）是完全不能看的，它并不能直接显示出颜色，即使通过**去马赛克算法**获得彩色图像，也会因CMOS的固定噪声和低灵敏度呈现出模糊的暗斑和亮点，这就需要通过ISP（Image Signal Processor，图像信号处理器）对原始数据进行处理，这会在后面的部分介绍

让我们回到CMOS传感器。理想情况下经过颜色滤光片，每个像素都只能感受对应颜色的光，但在追求光敏单元高密度的过程中，难免出现“像素间串扰”，这也是同时困扰CCD和CMOS的问题。

> 串扰（Cross Talk）被定义为：单个像素不能完全被一个颜色通道的光所激发的情况

我们使用另一种MEMS工艺来解决这个问题：**微透镜阵列**（Micro Lens）

![image-20240119020503332](三维重建1【计算摄影基础】.assets/image-20240119020503332.png)

通过在光敏元和颜色滤镜上再覆盖一层球状凸透镜阵列，可以让照射光敏元边沿的入射光折射回光敏元中央，从而让电荷包能够收集到所有光线

于是我们就得到了CMOS图像传感器*曾经*常见的堆叠结构：**正照**式（FSI）CMOS

很明显，正照式CMOS存在缺点：在每个像素位置，需要占用一定的面积来用作处理电路，因此CMOS的开口率远不如几乎能100%开口率的CCD。而为了提高开口率、增加感光面积，又需要挤占信号处理单元的空间，在老式CMOS传感器电路中，甚至只用一个BJT或MOS来做放大器，这就导致非常差的一致性，*固定模式噪声*严重；同时，像素尺寸的小型化也受到信号处理电路的限制，相同数量的像素需要更大的面积来布局

为了解决正照式的缺点，**背照**式（BSI）CMOS应运而生。它把感光像素与金属互联放置在硅片的两端来解决问题。正照式的微透镜让光线穿过硅片上方的多层Metal聚焦到硅片上的栅极，这需要让像素相隔很大距离才能实现；而背照式的光电二极管栅极可以紧密排布，透镜尺寸也可以缩小

在背照式传感器中，像素*背面*的整个面积都可以用来接受光线，因此开口率也和CCD传感器一样接近100%了。而CMOS传感器独有的信号处理电路放在像素正面一层。这样每个像素的信号处理电路相对不受面积制约，也能够获得更多金属层以支持更复杂电路的布线，因此可以做成比较复杂的放大器，比如带负反馈的差分运放，一致性会大大提高从而降低固定模式噪声。

不过背照式传感器对工艺提出了更高要求：常规半导体的厚度约为100微米左右；但背照式CMOS传感器厚度仅有3~4微米。正照式CMOS只需要保证受光面平整即可，对背面的均一性并无特殊要求；但背照式CMOS必须严格保证正反两面均极其平整

![image-20240119020927995](三维重建1【计算摄影基础】.assets/image-20240119020927995.png)

目前最新的设计是**堆栈式**照明（Stacked Illumination）CMOS。它将旁置的信号处理电路放到了底部支持基板上，实现在小尺寸传感器上集成更多的像素。由于像素部分和电路部分独立，像素部分可针对高画质优化，电路部分可针对高性能优化，甚至采用不同制成来获得更高性能和密度

![image-20240119021631306](三维重建1【计算摄影基础】.assets/image-20240119021631306.png)

### 模拟前端

![v2-a0116cfbbab88829b2dc7354a2bdc864_r](三维重建1【计算摄影基础】.assets/v2-a0116cfbbab88829b2dc7354a2bdc864_r.jpg)

现在我们终于从光电传感器来到了更熟悉（？）的数模混合电路。是时候讲讲模拟前端三大组件了。

首先我们能看到从传感器输出的所有信号会统一串行经过一个**精密运放**，这就是增益调节（**Gain**）。增益调节通常等效为对光圈的设置，这是很容易理解的

放大后的模拟信号会由一个精密高速**ADC**转换成通常是10bits~16bits的数字信号。这一部分是优秀CMOS设计的关键，SAR ADC（常常做成single slope架构）和pipeline ADC是最常见的两种，前者具有更高的精度（和性价比）、后者则具有更高的采样速率和带宽

最后，数字信号会通过查找表（**LUT**），它用于在一定的范围内修正传感器响应的非线性，同时还可以修复一些损坏的像素的输出

最后，经过修复的数字信号再进入接口控制器，就可以以DVP或者MIPI或者RAW格式输出给ISP器件了

## ISP和后处理

**图像信号处理器**（Image Signal Processor，**ISP**）是相机成像的核心环节，它负责接收图像传感器（Sensor）的RAW数据，并对其进行处理，主要包括线性纠正、噪声去除、坏点去除、去马赛克、内插、白平衡、自动曝光控制等算法。摄像头的成像质量很大程度上取决于ISP算法和参数调试如何

现代数字影像系统中，从CMOS/CCD传感器（即常说的“**Sensor**”）输出的RAW格式数据会通过MIPI/DVP等总线输出给**ISP**，但ISP只相当于一套算法的硬件加速器，其中每个算子的参数都需要预先输入，这就需要在ISP后再接入一个**AP**（应用处理器，Application Processor），AP使用I2C等总线控制ISP。

![image-20240203112554431](三维重建1【计算摄影基础】.assets/image-20240203112554431.png)

如上图所示，传统ISP芯片作为一颗独立的ASIC放在Sensor和AP之间，但目前很多图像SoC也都集成了ISP，新的方案被叫做**内置ISP**，对应的传统方案就是**外置ISP**。由于成本高、开发周期长等原因，外置ISP渐渐退出了市场。现在主流的方案已经变成Sensor直接接入一个**图像SoC**，这个SoC包含了ISP电路，还有片上的CPU、外设可以同时控制镜头、云台等附属设备。此外，图像SoC内置ISP、CPU、DSP......还有诸多其他片上外设，具有低成本、高集成度、高速通信的特点，能够有效减小板级布线难度、板层数、Layout面积。SoC供应商一般会提供整套ISP调参工具和API，这就在开发的难度也得到了降低。

> 外置ISP驱动的设计需要耗费更多成本（ASIC需要单独购买）、人力（开发难度大）和时间（调试周期长），摄像系统在PCB上占用的面积也会变大，这对追求影像小型化的现代光电设备来说是很沉重的负担；不过目前也有部分厂家在使用外置ISP方案，这些厂家一般都有*比较丰富的影像质量调试经验，能提供比传感器内置ISP更优秀的效果，并且有助于对产品线差异化*
>
> 目前手机多用内置ISP，相机多用外置ISP，不过随着图像SoC的发展，内置ISP将是大势所趋

ISP一般需要一个**固件**（调优参数表）来设置算法逻辑和参数，同时还包含一些外设来接收传感器的RAW图像输出，并控制镜头和传感器的状态，这就需要与CPU连接通信。在ISP和CPU的连接上，主要有三种方式：

* I2C/SPI：外置ISP多用，使用I2C控制ISP的寄存器，使用SPI下载ISP固件
* MMIO：Memory Mapped IO，内置ISP多见，直接将ISP内部寄存器映射到CPU内核地址空间
* Shared Memory：一部分内置ISP使用，CPU在启动后分配共享内存，ISP和CPU通过这段地址共享内存数据

> ISP需要同时控制传感器和镜头才能完成自动光圈AE、自动曝光AF、自动白平衡AWB等功能

一个ISP的固件包含三部分，一部分是内部各个算法IP的控制和参数库，一部分是AE/AWB/AF算法库，还有一部分是图像传感器控制库。通过预先对ISP进行参数调优获得一个参数表，将其固化到ISP内，即可实现大部分ISP功能；为了实现AE/AWB/AF功能，传感器控制库向ISP算法库注册回调函数从而适配不同的传感器和镜头

一个典型的ISP整体工作流程如下图所示，下面来逐步说明

![v2-c54998adc8b9506e7cbfb6aad9734683_r](三维重建1【计算摄影基础】.assets/v2-c54998adc8b9506e7cbfb6aad9734683_r.jpg)

### 读入RAW格式元数据

目前主流的CMOS传感器（下统称**Sensor**）都采用RGGB的RAW数据格式，常见的RAW精度有8/10/12/14bit等规格。

> 安防监控行业较多使用10/12bit精度的sensor，医疗行业则主要使用12bit以上精度的sensor，单反和广播电视行业则主要使用14bit精度sensor

对一些带宽和存储资源特别紧张的场合，有些sensor会支持压缩表示以节约带宽，但是这就需要ISP能够支持相应的压缩格式。4K分辨率的图像有800万像素，RAW格式则至少会占1600万字节（按8bit精度计算），YUV422格式占1200万字节，因此支持4K以上的芯片都会设计一个压缩算法并进行放缩和裁剪（Crop/Resize）

一般sensor和ISP通过MIPI总线（大数据量）或DVP总线（小数据量）进行连接，RAW数据通过这些协议传输到ISP的缓存区（帧缓存或行缓存，分别对应两种数据读入方式）

使用ISP处理图像数据时有两种常用的数据读入方式：

- **在线模式**（Online）：Sensor实时产生的像素数据和时序控制信号以行为单位送入。Online具备低延迟的优点，一帧图像的第一个像素数据流出sensor后马上就可以进入ISP流水线开始处理。
- **离线模式**（Offline）：待处理的图像以帧为单位存储在系统内存，处理时由控制逻辑通过DMA从内存中读取数据，并添加模拟sensor行为的时序控制信号，一起送给ISP进行处理。其优点在于吞吐量很大，一次能够完成一帧图像的处理；但ISP通常需要等到一帧图像的最后一个像素数据到齐之后才开始启动处理。

### 黑电平校正

图像数据为0时对应的信号电平被称为**黑电平**（Black Level）。上面说过Sensor输出的像素信号会受到暗电流影响，导致输出RAW数据不等于我们需要的黑电平值，这就需要**黑电平校正**（BLC，Black Level Correction）算法。比较常见的算法是已获得的图像信号值减去预标定的参考暗电流信号值

### 镜头阴影校正

受到光学镜头性能限制，相机在成像距离较远（或焦距较短）时，随着视场角慢慢增大，能通过照相机镜头的斜光束将慢慢减少，这就导致获得的图像中间比较亮，边缘比较暗，这被称为**渐晕**（Vignetting，也可以直接称为Luma Shading）。

![image-20240203121324605](三维重建1【计算摄影基础】.assets/image-20240203121324605.png)

> 渐晕的本质是Sensor接收到的通光量从中心向边缘逐渐衰减，让画面边缘亮度变暗。球面镜头常常会导致更明显的渐晕，平面镜头的渐晕则不那么明显

在一般情况下，渐晕不会像畸变那样导致图像看起来“不正常”，但在视场角很大的情况下，将导致图像边沿明显“变黑”，因此需要通过**镜头阴影校正**（LSC，Lens Shade Correction）算法处理。LSC的难点在于镜头边沿的光学性质不均还会导致*色差*（Chroma Shading）。在校正渐晕的同时还要考虑各个颜色通道的差异性。

> 色差即由于镜头对不同波长的光线折射率不同使得图像出现分开的色带的情况，分成横向色差和纵向色差，详细内容可以参考上文光学基础部分

我们从ISP算法的角度考虑，直接将色差和渐晕合称为镜头阴影。一般LSC算法首先确定图像中间亮度比较均匀的区域，将其作为基准，该区域内像素不需要做校正；根据基准计算出周边像素由于衰减带来的图像变暗的速度，从而计算出R、G、B三通道的*补偿增益*。计算中，以中心点为原心向周围以半径为单位逐次计算增益，这就使得距离中心点越远，增益补偿就越大。调参过程中，通过将镜头对准白色物体来进行LSC参数（补偿增益）的标定

当然，除了算法，还可以通过光学手段处理镜头阴影。一种处理微透镜的方法是让微透镜尺寸略小于感光面积，这样越靠近边缘，微透镜与感光面之间的错位就越大，从而补偿入射光线角度增大导致的焦点偏移

![image-20240203121849059](三维重建1【计算摄影基础】.assets/image-20240203121849059.png)

一般通过光学手段来减小色差，不过ISP中也可以通过LSC算法来处理——将全局RGB通道乘以预先标定的增益值就可以控制横向色差了。标定时一般只需要对多个关键点进行三个颜色平面的标定即可，任意像素位置的RGB通道增益可以使用相邻四个标定关键点通过双线性插值的方法动态计算得到

> 需要注意：镜头阴影是一种光学现象，LSC算法是针对目标镜头（Lens）而不是目标Sensor的。如果镜头发生变化，原则上需要重新进行LSC参数标定

**坏点**（Bad Point）即像素阵列中与周围像素点的变化表现出明显不同的像素。因为Sensor上通常有数百万、数千万的感光单元参与工作，因此出现坏点的概率很大

坏点可以分成三类：

* **死点**：一直表现为最暗值的点
* **亮点**：一直表现为最亮值的点
* **漂移点**：变化规律与周围像素明显不同的像素点

由于Sensor感光单元上存在拜尔阵列排布的彩色滤光片，每个像素只能得到一种颜色信息，缺失的两种颜色信息就需要从周围像素中得到。若图像中存在坏点，坏点造成的影响会随后续*去马赛克算法*的插值过程向外扩散，从而影响整个图像，因此必须在去马赛克之前消除坏点。

ISP中使用**坏点校正**（DPC，Defect Pixel Correction或称Bad Point Correction）算法实现。DPC首先在RGB域上做nxn像素的评估，若某个点和周围的点偏离度超过阈值，则判定为该点是坏点。为防止误判，也可以使用多帧连续评估的逻辑来判别。通过这样nxn像素区域的遍历最后得到所有坏点位置。判定区域内出所有坏点的位置后，对找到的坏点做中值滤波，替换原来的值即可。判别结果可以在标定时生成，使用过程中再进行校正

### 绿平衡

由于感光器件制造工艺和电路问题，拜尔阵列中靠近R像素和B像素的两个G像素（Gr、Gb）输出会存在差异，导致图像出现**迷宫格子现象**。如下图

![image-20240203123225894](三维重建1【计算摄影基础】.assets/image-20240203123225894.png)

ISP中使用**绿平衡**（GB，Green Balance）算法，通过均值滤波处理Gr、Gb存在的差异，同时保留高频信息

### RAW域去噪

![v2-c54998adc8b9506e7cbfb6aad9734683_r](三维重建1【计算摄影基础】.assets/v2-c54998adc8b9506e7cbfb6aad9734683_r.jpg)

ISP是一个非常长的流水线，这从上面的算法流程图中就可以看出来。内部数字信号噪声会在各个模块中产生、传播，影响图像质量，因此主流的ISP一般选择在RAW Domain、RGB Domain、YUV Domain各放置**去噪**（Denoise）模块以控制不同类型和特性的噪声。

Sensor输出的RAW图像本身就携带了噪声（包括热噪声、光散粒噪声、读出噪声、固定模式噪声等等），这些噪声是最后输出图像中噪声的主要来源。当Sensor温度不在合适范围内、增益较大、环境较暗情况下，噪声就会更加明显，成为影响图像质量的主要因素

> Sensor输出的图像每次会经过ISP某个算子模块处理后也会引入一些新的噪声，或者对原有噪声进行了放大。比如上面说过的LSC算法，其实质就是将输入图像乘上一个与像素位置有关的增益系数。根据噪声传播的基本原理，当增益系数大于1时，图像中的噪声是与信号一起被同步被放大的。此外，由于ISP所用乘法器精度有限，每做一次乘法就会重新引入一次截断误差，这就是LSC算法的”*本征噪声*“。所以经LSC处理后图像的整体噪声水平会有所增加

由于**RAW图像中相邻像素点分别隶属不同颜色通道**，所以相邻像素之间的相关性较弱，**不具备传统意义上的像素平滑性**，因此**很多基于灰度图像的降噪算法都不能直接使用**；**RAW数据每个像素点只含有一个颜色通道的信息**，所以**很多针对彩色图像的降噪算法也不适用**

噪声会使图像整体变得模糊，丢失细节，所以需要对图像进行去噪处理。传统上会采用在空间域（**空域**）、时间域（**时域**）上的去噪算法，但随着深度学习技术发展，基于深度学习的降噪算法也开始出现，不过由于这些算法对算力要求都比较高，难以达到高分辨率下的实时视频处理，因此在产品中应用不广。需要根据噪声的主要来源来确定降噪算法

* **热噪声**：在负载电阻（CMOS器件）中，自由电子存在随机的热运动，会引起电阻两端电压的波动，这在CMOS制成缩小的情况下愈发明显，会呈现为典型的白噪声特征叠加在输出图像信号中
* **固定模式噪声**（FPN，Fix Pattern Noise）：CMOS中每个感光单元都配有独立的放大器，但是由于每个感光单元都有不同的制造偏差，这就造成像素输出信号不匹配，从而出现噪声。通常采用**双采样降噪**来处理。在光线较暗的环境下，画面会由于暗电流值不同而出现明显噪点，这时通过对景物进行两次不同曝光度和敏感度的采样，再将两次采样结果进行运算就可以处理好FPN了
* **光散粒噪声**：在感光阵列光电二极管中，会出现由于光电发射过程产生的具有泊松分布的噪声——曝光时光子转化成电荷包内电荷（的等效粒子数）是不均匀的，光电转化是遵循费米子热力学规律的随机过程（数学上遵循泊松分布）。而其功率谱密度在所有频率上恒定，也就是说光散粒噪声也是白噪声。对于比较明亮的场景以及较大尺寸的传感器，光散粒噪声是最主要的噪声来源
* **读出噪声**：Sensor内部，当感光单元阵列输出的信号进入模拟前端后，会由于精密运放和高速ADC而引入另一部分噪声，这些噪声就是读出噪声。读出噪声包含由运放引起的热噪声、1/f噪声、本征噪声，还包含由ADC引起的ADC量化噪声

RAW域降噪算法会将RAW图像按照颜色分成R、Gr、Gb、B四个通道，然后在每个通道上分别应用滤波器进行平滑，常见的滤波算法包括：

* **经典低通滤波器**：在传统DSP领域出现的均值滤波、中值滤波、高斯滤波、维纳滤波等都可以推广到矩阵形式，从而应用在图像上。这些方法比较简单、占用资源少、速度快，但由于滤波器各向同性，可能会破坏图像的边缘细节，并且由于没有考虑颜色通道之间的相关性，就容易引入伪彩等噪声
* **非线性去噪算法**：考虑相邻像素和本像素的相似程度，对于相似度高的像素给予更高的权重，从而避免丢失边缘的情况出现，复杂度增加也不算大。常见Eplison滤波、双边滤波器等。
* **引导滤波器**：对于任一颜色通道的图像，以当前位置像素为中心，在一个固定大小的滤波窗口内的所有邻近像素计算权重，计算权重的方法是以某个*引导图像*作为参考，在引导图像的对应滤波窗口内，凡是与像素性质“类似”的像素都得到较大的权重，“性质相反”的像素则得到较小的权重。该算法需要一个引导图像作为参考，基于“待滤波图像各通道的颜色梯度分布与引导图像是一致的”这一假设，如果假设不成立反而会导致滤波效果变差、引入额外的噪声
* **基于块匹配的滤波算法**：利用图像的自相似特性，在以当前像素为中心的一个滤波窗口内找到与当前块最相似的几个块，当前像素的滤波值即等于几个相似块的中心像素的加权平均值。常见的应用包括非局部均值滤波和BM3D算法。特点是平滑性能和边缘保持性能很好，但计算量很大，不适合处理实时视频
* **3DNR**：结合空域滤波和时域滤波的一种降噪算法。检测视频的运动水平，对图像像素进行空域滤波和时域滤波的加权后输出

### 自动白平衡

**白平衡**（WB，White Balance）与**色温**相关，用于衡量图像的色彩真实性和准确性。简单的说，自动白平衡就是通过图像调整，使在各种光学条件下拍摄出的照片色彩和人眼所看到的景物色彩完全相同。 

首先来介绍一下**色温**（Color Temperature）。色温是一种定量地以开氏温度表示色彩的方式。最早开尔文提出：假定一黑体能够将落在其上的所有热量吸收而没有损失，同时又能将热量生成的能量全部以光的形式释放出来，它便会因受到热力的高低而变成不同的颜色

700K之下的黑体所放出来的辐射能量很小且辐射波长在可见光范围之外，看起来是黑色的。黑体的温度高过700K的话，它会开始变成红色；随着温度的升高，黑体将呈现橙色、黄色、白色......温度越高，光色越偏蓝。当温度超过1600K时开始发白色和蓝色，同时会放出大量的紫外线。可以发现，黑体吸热放出电磁波的过程遵循光谱，其轨迹可以用普朗克轨迹（或称为黑体轨迹）描述。因此较高温度的黑体靠近光谱结尾的蓝色区域；较低温度的黑体靠近红色区域

人眼具有独特的适应性——我们有的时候不能发现色温的变化。但相机的Sensor不同，如果摄像机的色彩调整同景物照明的色温不一致，就会发生偏色，*白平衡正是为了避免偏色的出现*。

> 例如白炽灯照明下拍出的照片易偏黄；而在户外日光充足则拍摄出来景物会偏蓝
>
> ![image-20240203234956189](三维重建1【计算摄影基础】.assets/image-20240203234956189.png)

**自动白平衡**（AWB，Automatic White Balance）算法的工作就是模拟人类视觉系统的颜色恒常性特点来消除光源颜色对图像的影响，*让不同色温光线条件下白色物体的输出都转换为更接近白色*

![image-20240203235017558](三维重建1【计算摄影基础】.assets/image-20240203235017558.png)

自动白平衡通过改变R、G、B三个通道电平的平衡关系，使反射到镜头里的光线都呈现为消色，最终让R、G、B三个通道的单通道总和相等，一般在实际应用中会选取一个假设，对应实现算法。

比较常用的AWB算法有：

* **灰色世界假设**（Gray World Assumption）：假设图像的平均颜色是白色

    灰色世界假设算法将图片中所有像素值取平均后的值设置为白色。于是对应平均值的所有像素点都被当成了白色。显而易见，这会导致图片中偏暗和偏亮的部分白平衡不准；但该算法的计算量不高，使用较为方便

    算法公式如下图

    ![image-20240203235358279](三维重建1【计算摄影基础】.assets/image-20240203235358279.png)

* **白世界假设**（White World Assumption）：假设图像中最亮的区域是白色

    白世界假设衍生出的算法又称为完美反射算法。该假设下，白色是反射率最高的颜色，而RGB值之和最高的像素就被作为了白色。这个算法看起来比灰色世界假设要“科学”，但如果图像里没有白色或者存在比较强的噪声，这个算法的效果反而不如灰色世界假设

    算法公式如下图

    ![image-20240203235410202](三维重建1【计算摄影基础】.assets/image-20240203235410202.png)

在实现上，AWB算法分为三个关键步骤：首先要根据假设检测色温，为减少计算量，通常选取某个特定区域像素进行计算，因此如果图像颜色较为单一或选定区域正好落入大的色块，按照算法自动计算的色温会非常不准确，这就需要再额外设计约束条件，挑选出白色像素来计算。第二部是计算增益，为了减少计算量，一般只会调整R和B通道的增益，最终的目标是R=G=B。最后就是根据增益调整整幅图片的色温，即将每个通道的增益值与对应通道的像素输出值相乘，完成调节

除了AWB算法，一般ISP还允许**精确白平衡**和**分档白平衡**两种模式。精确白平衡模式下，用户首先使用标定的白纸充满视野，然后进行白平衡调节得到目标增益，相当于完成标定后再拍摄，这样的白平衡可以做到非常准确，但需要用户程序支持甚至用户手动调节，十分不便。分档白平衡模式下，ISP会内置不同光源和色温值对应的白平衡增益，比如设置日光、阴天、日光灯、白炽灯等档位，或者开氏温度值，用户在拍摄前自行选择需要的白平衡档位，从而完成调节

### 去马赛克

**去马赛克算法是ISP的核心功能之一**。在去马赛克之前，ISP处理的都是RAW格式数据，它本质上是一个代表每个单元灰度的二维矩阵，图像的颜色通道是由Sensor的拜尔阵列（或其他排列的彩色滤光片矩阵）赋予的，去马赛克算法就是要将RAW格式的图片转换成每个像素都带有三个通道颜色信息的**RGB格式**图片

经过CFA（Color Filter Array，彩色滤光阵列）作用后，每个像素点只能感应到一种颜色的亮度（灰度值），而CFA是呈阵列排布的，因此可以利用周围像素点的值来获得该点其它两个通道的值。一般来说去马赛克（Demosaic）算法就是通过**插值**将每个像素所代表的真实颜色计算出来。

常见的插值包括：

- **双线性插值**
- **双三次样条插值**
- **Edge-Aware插值**

去马赛克算法的主要难点在于RAW域的任何一个像素都只包含一个真实的采样值（R、G或B），构成像素（R、G和B）的其它两个值需要从周围像点中预测得到。预测一定是不准的！这就需要使用更复杂的算法来修正

这里只以最简单的双线性插值为例介绍。首先取待解算像素的四个邻域，根据R、G、B通道像素的排列有不同的取法，如下图所示。

![image-20240204003656584](三维重建1【计算摄影基础】.assets/image-20240204003656584.png)

然后对邻域像素的值做平均，就可以得到待解算像素的(R,G,B)值了

![image-20240204003542079](三维重建1【计算摄影基础】.assets/image-20240204003542079.png)

通过去马赛克算法，我们就能从灰白、单维度的RAW域进入彩色、三维度的RGB域

### 串扰

串扰是两条信号线之间的寄生电容、寄生电感导致的信号干扰。具有串扰的两条信号线，一条的跳变沿会在另一条上引起尖峰脉冲。

在Sensor电路中也存在串扰（参考之前CMOS/CCD部分提到过的串扰问题）。主要包括三种形式，如下图所示

![image-20240204005004814](三维重建1【计算摄影基础】.assets/image-20240204005004814.png)

最关键的串扰是**RGB频谱串扰**（Spectral Crosstalk）——由于滤光片并不能完全滤除所有不需要的颜色光（甚至还可能出现色偏，滤光片透光的波长并不是“纯色”，而是有点偏差的），会有来自其他通道（其他波长）的光子照射到感光单元，这是不可避免的问题

其次是由于电荷包存储电荷溢出导致的**电子串扰**（Electronic Crosstalk），当满阱容量不足以容纳曝光带来的大量光电荷后就会出现大量电子串扰。增加满阱容量或引入像素间绝缘可以避免该问题

最后是由于光线入射角问题产生的**空间串扰**（Spatial Crosstalk）。当光线入射角度大于MEMS微透镜能够聚光的最大角度时，由于CMOS的结构限制，光子会在内层电介质中照射到相邻的像素或其他无效区域。随着CMOS工艺制程提高，像素间距越来越小，在没有隔离层的情况下，空间串扰已经变得相当严重

### 颜色校正

AWB负责较准白色，**颜色校正**（CCM，Color Correction Matrix）就是负责校正白色以外其他颜色的，主要矫正对象是在滤光片处各颜色像素间的颜色渗透带来的颜色误差。、

首先将该Sensor拍摄到的图像与一幅*标准图像*相比较，计算得到一个校正矩阵。在后续拍摄中，就可以利用这个矫正矩阵对传感器拍摄的所有图像进行校正，从而优化成片的色彩细节

一般来说颜色校正的过程会伴随着对颜色饱和度的调整

### 自动曝光

相机的曝光影响了图像的明暗程度。人眼有明暗自适应的能力，因此可以适应白天夜间的不同场景，但图像传感器却不具有这种自适应能力，这就需要使用**自动曝光**（AE，Automatic Exposure）算法来确保拍摄的照片获得准确的曝光从而具有合适的亮度。

AE模块使用到自动测光系统来确定当前图像的曝光量，再自动配置曝光三要素——镜头光圈、Sensor快门和读出电路增益来获得最佳的图像质量。对于算法，会根据曝光三要素的优先级区分成三类：

* **光圈优先**：算法会优先调整光圈到合适的位置，再分配曝光时间和增益。特点是能均衡噪声和景深
* **快门优先**：算法会优先分配曝光时间，再分配Sensor增益和ISP增益。这样拍摄的图像噪声会比较小
* **增益优先**：算法优先分配Sensor增益和ISP增益，再分配曝光时间。更适合拍摄运动物体的场景

在自动曝光算法中，测量和场景分析是关键算法。测量算法利用图像的曝光信息来获得当前光照信息，按照统计方式的不同，可以分成*全局统计*、*中央权重统计*、*加权平均统计*；在部分实现中，为了获得更准确的光照信息，还会加入一个光照传感器，专门得到当前环境的光照特征。场景分析算法则是在当前光照信息的基础上提升图像质量，是自动曝光算法的关键（厂家“调参”核心技术）。很多厂家的ISP没有配备场景分析算法实现，因为场景分析技术依赖模糊逻辑和神经网络算法匹配各种不同光照条件，ISP的算力要达到很高才能实时处理。

在使用上述算法得到当前的环境曝光量以后，就可以对曝光时间和曝光增益进行设置，从而调整曝光量。在进行曝光和增益调整的过程中，一般都是变步长来调整的，这样可以提高调整的速度和精度：当前曝光量与目标量差别在*需求范围阈值1*以内的时候，说明曝光已经满足要求，不需要进行调整；差别在阈值2内时，说明当前曝光与要求的光照有差别，但差别不大，只需要用较小的步长来进行调节；当差别在阈值2以上的时候，则表明差别较大，需要用较大步长来进行调节。通过设置阈值1、阈值2就可以划定补偿了

### 自动对焦

在现代ISP中，**自动对焦**（AF，Automatic Focus）算法也被引入，让用户无需手动控制变焦镜头——这在很多微型影像模组（比如手机摄像头）中是不可能的事

AF算法首先判断图像的模糊程度，设置一个模糊度损失函数来对每张图像进行逐次评价，然后通过搜索算法得到一系列评价值，在这个过程中不断微调镜头并采集新图片，试图获得范围内最低损失函数值（或者说最高评价值）点，从而确定对焦距离，最后就可以移动镜头调节到计算出来的合适焦距了。

其中的损失函数（或者说评价函数）是关键点，需要考虑到频域特征和灰度分布。而搜索算法可以使用爬山算法、动态规划算法、梯度下降法等常见算法

### 宽动态范围（HDR）

自然界的中光强度很宽，而人眼对高亮、极暗环境的细节分辨能力相对较窄

> 当然摄像头的分辨范围更窄

而**HDR**即High Dynamic Range，**宽动态范围**追求的就是将记录视觉范围内高亮、极暗环境中的细节分辨清晰，目标就是解决数字影像设备在宽动态场景中采集的图像出现亮区域过曝而暗区域曝光不够的现象。这是一个既需要HDR专门算法优化，也需要Sensor基础参数过硬的需求

**动态范围**（DR，Dynamic Range）指Sensor支持的最大输出信号和最小输出信号的比值，或者说图像最亮部分与最暗部分的灰度比值，一般用分贝db描述。普通CMOS的动态范围在60db（1000:1）左右，而专门对HDR做优化的CMOS动态范围能达到65-75db（5600:1）

在晴朗夏天室外光照可达10~20万lux，理论上需要5000:1的动态范围为，在Sensor、ISP内部至少需要使用13位的数据才能处理。考虑到CPU中采用int16格式更加方便，并且还需要预留多个小数位保证乘除法、乘方、指数运算，因此主流ISP方案都在用20位浮点/整型的格式来优化HDR

当然，HDR对显示设备也有要求——一般的显示器只能提供256级灰阶，也就是uint8格式，对应动态范围为100:1；而要符合HDR10标准的显示器需要至少提供1000:1的动态范围，很明显不太能匹配CMOS的动态范围，因此ISP为了支持HDR，还需要完成从5000:1到1000:1甚至100:1的动态范围压缩。一般采用**逐级压缩**策略：数据在HDR模块被压缩到12bit精度，经Gamma校正（后面会介绍）后再进一步压缩到10bit精度，在最后的颜色空间转换后压缩到8bit，最终以RGB888/YUV444的格式（甚至有些ISP会输出YUV422、RGB565）输出

其中从16/20bit压缩到12bit的过程被称为**色调映射**（Tone Mapping），这也是HDR模块中主要实现的算法，**需要尽量保证图像细节不损失**，使得图像显示出更多的信息

一般色调映射算法的思路就是将像素值在特别暗的区域拉高，在特别亮的区域拉低，可以分成两种：

* 全局色调映射（Global Tone Mapping）：设置一个统一的色调映射曲线，对整幅图的高亮区压低亮度，低亮区抬高亮度。存在局部对比度下降导致图像“蒙上一层雾”的缺点

    > 有研究者构造了双边滤波函数来保证在图像局部边缘处不进行色调映射以保持局部细节的方式

* 局部色调映射（Local Tone Mapping）：通过多帧相加确定哪些区域是高亮区，哪些区域是低亮区，分区进行处理

> 顺便一提，这一步实际上也是在构造Gamma曲线。在下一节可以感受到Gamma函数的强大

### Gamma校正/增强

Sensor对入射光的响应大部分是线性的，人眼对入射光的响应也是线性的。但人对光线的感觉却是非线性的（大致成对数关系）——人眼对暗部细节更加敏感，这种*敏感度很接近Gamma函数*；此外，显示器的显示效果也是非线性的，对亮光信息更加敏感，这就导致一个线性、两个非线性要叠加起来，形成一个非常复杂的映射

![image-20240204011252404](三维重建1【计算摄影基础】.assets/image-20240204011252404.png)

> 伽马校正的最初起源是CRT显示器的非线性：CRT显示器使用阴极射线管进行成像，由于发光强度和输入电流之间的函数成指数，这很容易导致成像扭曲，因此需要先进行校正。Gamma校正的名字就此沿用下来

**Gamma校正**（Gamma Correction），也叫**色调重建**。进行校正的过程就是对图像的**Gamma曲线**进行编辑，检出图像信号中的深色部分和浅色部分，使两者比例增大，从而提高图像对比度效果，增加更多的暗部色阶

Gamma曲线是一种特殊的色调曲线，当Gamma值等于1时曲线为斜率等于tan(45°)的直线；值高于1时，Gamma将会造成输出暗化，曲线接近0的部分斜率大于tan(45°)，高光部分被压缩、暗调部分被扩展；值小于1时，Gamma曲线接近1的部分斜率大于tan(45°)，高光部分被扩展而暗调部分被压缩，这会造成输出亮化。

人眼是按照Gamma<1的曲线处理输入图像；显示器则是按Gamma>1的曲线处理输入图像。叠加作用下，我们期望Gamma值接近1。由于函数的解析解非常复杂，我们只能退而求数值解，也就是构建一个Gamma校正查找表（LUT），将将不同亮度范围的理想输出值在查找表中设定好，处理图像时根据根据输入的亮度得到理想的输出值。这个LUT需要精密地标定，是很多相机厂商调色的核心技术

此外，需要注意：ISP的很多步骤是可以互换的，但Gamma校正必须在图像压缩前完成。这是因为图像压缩采用的是有损压缩，会导致部分色彩信息丢失，如果在压缩后进行Gamma校正，后续在图像解压缩时就会出现色彩恢复不正确的问题

> Gamma校正和HDR有时会合并到同一步骤实现，但算法会变得更复杂，从而导致硬件实现更难

### 自动光圈

**自动光圈**（AI，Auto Iris）控制算法是配合自动曝光算法一起使用的。

算法通过控制镜头中的步进电机推动光圈挡光片来精确控制光圈大小，主要目的是设置最佳光圈位置以便入射光能投射到Sensor中心附近光学效果好、误差小的区域，从而提高图像质量

比较常见的算法为**P-Iris**。算法需要接收当前增益、曝光时间和光圈所在位置作输入，增益和曝光时间都从自动曝光部分传入，而光圈位置则根据执行器原理有所区别：如果使用步进电机驱动光圈机械结构，则可以直接堆控制电机走的步数来计算当前位置；如果使用直流电机控制光圈，则需要用霍尔传感器或光电编码器来检测当前位置。一般来说自动光圈只需要在自动曝光无法再增加增益和曝光时间的情况下微调光圈到目标位置即可

### RGB2YUV颜色空间转换（CSC）

Sensor输出的RAW数据是单通道灰度数据，经去马赛克后变成三通道RGB数据，上面的算法都是在RGB域进行的，但**有些处理在YUV格式下更方便**，而且**YUV存储和传输时更省带宽**，因此常常在完成所有RGB域处理算法后将RGB数据转换成YUV数据，进入YUV域，这个过程被称为**颜色空间转换**（CSC，Color Space Conversion），对应的算法为RGB2YUV

**YUV**是不同于RGB的一种基本色彩空间。RGB空间通过三原色红绿蓝的亮度（灰度）叠加来描述所有色彩；而YUV通过Y-**亮度**、U-**色度**、V-**饱和度**来描述所有色彩。人眼对于亮度的敏感性远比对色彩大很多，因此对人眼而言Y分量比U、V分量重要得多

> RGB、YUV、CMYK是三大基本色彩空间。CMYK常用于印刷出版领域，很多打印机就采用CMYK进行印刷

基于这一基本理念，和RGB555、RGB888类似，YUV家族中也有**YUV444**、**YUV422**、YUV420等格式，这些格式有些比原始RGB图像格式所需内存要小很多，通过分别存储亮度和色度分量来为视频编码压缩图像带来一定好处。

在数字影像系统中，YUV常以**YCbCr**格式（缩写为*YCC*）存在，其中Y指亮度分量，Cb指蓝色色度分量（蓝色色差），Cr指红色色度分量（红色色差），**JPEG、MPEG均采用该格式**，因此一般人们所讲的YUV大多是指YCbCr，不过严格来讲它是YUV经缩放和偏移的改动版。Cb、Cb表示的都是色差，即蓝色和红色部分与RGB输入信号亮度值之间的差异

在YUV域，可以进行**明度对比度控制**和**色调控制**这两个难以在RAW域或RGB域实现的算法。前者就是通过Gamma函数对Y、UV分量进行微调；后者是对U分量单独进行可变增益调节

### YUV域去噪

在数据进入YUV域后，还需要对之前RGB域和RGB2YUV算法种产生的图像噪声进行滤波，即**YUV域去噪**。

我们可以把YUV图像噪声分为亮度噪声（Luma Noise）和色度噪声（Chroma Noise）。一张RGB空间的图片携带的噪声转换到YUV域，在Y通道上附加的噪声即**亮度噪声**，在UV通道上附加的噪声就是**色度噪声**

> 需要区分色度噪声和RGB域的颜色噪声，颜色噪声并不是指图片的色彩上叠加的噪声，而是每个颜色通道上亮度值上叠加的噪声，因此颜色噪声可以视作图片空间域噪声；但色度噪声就是指图片的色彩上叠加的噪声，它应该归于频域噪声

亮度噪声通过和RAW域去噪类似的滤波算法就可以滤除；色度噪声一般有两种处理方法：

* 自动曝光会使用不同的ISO值拍摄图片，当ISO值提高一倍时，快门时间也缩短一倍，从而保证图像灰度均一。当快慢时间非常短而ISO很高时就会产生较多噪声，通过控制快门时间和ISO可以从源头上降低色度噪声
* 自动曝光按照一定曝光值和一定曝光增益增益进行拍摄，以此为基准在后面增益提高1倍时，曝光值也降低1倍。在曝光增益很高时也会产生较多噪声，因此可以通过不同设定控制曝光增益的标准差倾向来分析出差异，再使用滤波算法针对差异滤波

### 放缩和裁剪

完成所有处理后，就可以对图像进行放缩（Resize）和裁剪了。

这里使用到的算法常常是传统的机器视觉算法，包括**上采样**和**下采样**两大类。**上采样指的是将图片分辨率放大，下采样指的是将图片分辨率缩小**。上采样一般通过插值方法在现有像素之间增加新的像素来实现，可以增加图像的细节和清晰度，但并不会改变图像的内容。常见算法包括最近邻插值、双线性插值、三次样条插值等。下采样则一般通过减少图像中像素数目实现，会导致图像细节丢失、信息模糊。常见算法包括平均池化（AvgPool）和最大池化（MaxPool）

在最新的实现中，上采样算法演变成了超分辨率算法，通过卷积或生成对抗网络来获得新的信息。基于深度学习的算法效果很好，但对算力要求更高。部分SoC采用专门的加速器在ISP外对图片进行超分辨率操作

### 编码

最后，我们得到了完整图像的所有数据，只需要进行**JPEG**或**MPEG**编码后就可以直接存储。如果需要实时显示，则需要将数据通过MIPI-CSI总线或DVP总线传输给外部CPU

JPEG编码是最常见的图像压缩编码格式，它接受YCbCr格式的数据输入，首先对色度份量进行降采样（比如把YUV444降采样为YUV420）进行初步压缩；随后算法将图像分成8x8的图像块，对每个图像块进行独立的离散余弦变换（DCT），把图像块从空间域转换到频域，提取其频域特征后再进行二次量化；二次量化是针对DCT系数进行的，这一步的量化系数决定了JPEG图像的压缩比和图像质量；JPEG支持使用预测编码进一步减少数据冗余，它利用相邻像素之间的相关性对差异进行编码；最后，JPEG会使用霍夫曼编码对量化后的DCT系数或预测编码结果进行三次编码，霍夫曼编码利用数据中的统计特性，将出现频率高的符号用较短的编码表示，出现频率低的符号用较长的编码表示，从而进一步压缩数据规模

MPEG编码是一种常见的视频压缩编码格式。它基于帧间预测的概念，利用视频序列中连续帧之间的冗余性，通过对当前帧与之前的参考帧进行运动估计和补偿，可以计算出*残差帧*（当前帧与参考帧（一般是上一帧）之间的差异）；首先对残差帧进行DCT编码，将残差帧转换为频域的DCT系数，随后和JPEG一样对系数进行量化，这一步的系数也决定了MPEG的质量和数据规模。随后使用霍夫曼编码对数据进行二次编码，进一步压缩。此外，MPEG算法还会对帧间预测中的运动向量进行编码，以表示当前帧相对于参考帧的运动信息；也会对关键帧进行帧内编码，仅利用当前帧自身的信息进行压缩，这两种编码会依靠残差帧和每帧当前的信息进一步压缩数据量

> 需要注意：JPEG和MPEG编码都是有损压缩，它们会有选择性地丢失部分图像信息。

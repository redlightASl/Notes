# 将YOLO部署到硬件设备

承接上篇博文内容，本篇介绍花式部署YOLO算法，不过重点还是在于嵌入式平台，主要是k210、stm32和FPGA

## 带GPU的平台（Desktop或Nvidia Jetson系列）

对于带GPU的平台，部署 YOLO是很方便的，我们只需要配置Cuda，安装pyTorch、TensorFlow/Keras、PaddlePaddle、Caffe等框架，把自己的模型搬进来先训练，保存参数后执行推理；抑或是获取到现成的模型，转换成ONNX、TensorRT等推理框架支持的形式，直接执行。

这一部分主要以很常用的YOLOv5（v6版本）软件代码为例介绍YOLO文件中比较重要的项目

### 数据集和yaml模型配置文件







### 工具目录utils

















### train.py

















### detect.py















### export.py







## 带NPU的平台（K210）



















## 只有CPU的SoC平台（树莓派）

在树莓派等只有CPU的嵌入式SoC平台上部署神经网络有两种方式：一种是使用TensorflowLite这样支持CPU的轻量级推理框架；另一种是使用NCNN这样专为CPU设计的神经网络框架。两种方法都需要提前针对框架进行模型的剪枝、融合、量化















## 算力不足的MCU平台（STM32）

在算力不足的MCU平台上很难部署大规模的神经网络，不过目前很多机器人应用中也已经引入了十层以内乃至十余层卷积的中小规模神经网络。尽管如此，还是需要MCU性能不能太低，并且需要使用专用的基于C语言的推理框架（比如Tiny-ML），还要灵活使用MCU提供的DSP、FPU核配合汇编进行数据处理加速，涉及到很多方面，因此相对较难。

下面使用ST推出的Cube.AI工具部署LeNet-5模型到stm32h750vb上

### 模型训练







### 模型量化和Cube.AI工具









### 部署及修改代码











### 运行和优化









## 具有硬件加速的其他平台（纯FPGA）















# 参考教程




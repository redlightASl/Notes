# 常见的神经网络结构

本篇博文将全面介绍从多层感知机（MLP）到Transformer的主流神经网络结构，内容以*机器视觉*（CV）领域的算法为基本骨架，原则上按照时间顺序进行梳理，旨在梳理每篇关键论文对模型的核心改进点，并结合硬件给出量化推理/硬件化部署的基本方案。由于着重于边缘端部署，本文将不会提及视觉大模型相关内容。

## Hidden Layer和MLP

> Rumelhart D E, Hinton G E, Williams R J. Learning representations by  back-propagating errors[J]. nature, 1986, 323(6088): 533-536.



### Linear



### Sigmoid



## CNN

> LeCun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to  document recognition[J]. Proceedings of the IEEE, 1998, 86(11):  2278-2324.



### Conv-BN-Act

> AlexNet



### Shortcut

> ResNet



### PointWise-DepthWise Conv

> MobileNetv2



### Partial Info

> CSPNet GhostNet FasterNet



## RNN和LSTM

> Hopfield J J, Tank D W. “Neural” computation of decisions in  optimization problems[J]. Biological cybernetics, 1985, 52(3): 141-152.
>
> Hochreiter S, Schmidhuber J. Long short-term memory[J]. Neural computation, 1997, 9(8): 1735-1780.



## GNN

> Scarselli F, Gori M, Tsoi A C, et al. The graph neural network model[J]. IEEE transactions on neural networks, 2008, 20(1): 61-80.



## encoder&decoder

> Cho K, Van Merriënboer B, Gulcehre C, et al. Learning phrase  representations using RNN encoder-decoder for statistical machine  translation[J]. arXiv preprint arXiv:1406.1078, 2014.



## GAN

> Goodfellow I, Pouget-Abadie J, Mirza M, et al. Generative adversarial  nets[J]. Advances in neural information processing systems, 2014, 27.



## Attention

> Bahdanau D, Cho K, Bengio Y. Neural  machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014.



## Transformer和Mamba

> Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J].  Advances in neural information processing systems, 2017, 30.



### Embeddings



### Self-Attention



### Multi-Head Attention



### Mamba



## ViT





## 其他参考资料

https://zhuanlan.zhihu.com/p/644344950

